{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Usage\" data-toc-modified-id=\"Usage-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Usage</a></span><ul class=\"toc-item\"><li><span><a href=\"#Just-do-it:-A-minimal-boilerplate-facade-to-OpenAI-stuff\" data-toc-modified-id=\"Just-do-it:-A-minimal-boilerplate-facade-to-OpenAI-stuff-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Just-do-it: A minimal-boilerplate facade to OpenAI stuff</a></span></li><li><span><a href=\"#Raw-form\" data-toc-modified-id=\"Raw-form-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Raw form</a></span></li></ul></li><li><span><a href=\"#Scrap\" data-toc-modified-id=\"Scrap-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Scrap</a></span><ul class=\"toc-item\"><li><span><a href=\"#OpenAPI-OpenAI\" data-toc-modified-id=\"OpenAPI-OpenAI-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>OpenAPI OpenAI</a></span></li><li><span><a href=\"#Align-OpenAPI-specs-and-openai-python-functions\" data-toc-modified-id=\"Align-OpenAPI-specs-and-openai-python-functions-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Align OpenAPI specs and openai python functions</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Scrap-to-make-raw\" data-toc-modified-id=\"Scrap-to-make-raw-2.2.0.1\"><span class=\"toc-item-num\">2.2.0.1&nbsp;&nbsp;</span>Scrap to make raw</a></span></li></ul></li><li><span><a href=\"#specs/shema-scrap\" data-toc-modified-id=\"specs/shema-scrap-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>specs/shema scrap</a></span></li><li><span><a href=\"#Schemas\" data-toc-modified-id=\"Schemas-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Schemas</a></span></li><li><span><a href=\"#Paths\" data-toc-modified-id=\"Paths-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Paths</a></span></li></ul></li><li><span><a href=\"#Engine/Model-list\" data-toc-modified-id=\"Engine/Model-list-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Engine/Model list</a></span></li><li><span><a href=\"#illustrating-a-story\" data-toc-modified-id=\"illustrating-a-story-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>illustrating a story</a></span></li><li><span><a href=\"#Making-an-Aesop's-Fables-book\" data-toc-modified-id=\"Making-an-Aesop's-Fables-book-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Making an Aesop's Fables book</a></span></li><li><span><a href=\"#illustrating-concepts\" data-toc-modified-id=\"illustrating-concepts-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>illustrating concepts</a></span></li><li><span><a href=\"#open-ai-tutorial-examples\" data-toc-modified-id=\"open-ai-tutorial-examples-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>open ai tutorial examples</a></span></li><li><span><a href=\"#chatGPTs-code-for-illustrate\" data-toc-modified-id=\"chatGPTs-code-for-illustrate-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>chatGPTs code for illustrate</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T09:20:44.061683Z",
     "start_time": "2023-05-07T09:20:44.038299Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just-do-it: A minimal-boilerplate facade to OpenAI stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the typical tasks you might want to use OpenAI for.\n",
    "\n",
    "Note there's no \"enter API KEY here\" code. That's because if you don't have it in the place(s) it'll look for it, it will simply ask you for it, and, with your permission, put it in a hidden file for you, so you don't have to do this every time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T23:16:14.998070Z",
     "start_time": "2023-04-22T23:16:07.779598Z"
    }
   },
   "outputs": [],
   "source": [
    "import oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T23:17:55.674679Z",
     "start_time": "2023-04-22T23:17:54.837861Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " chatbot based on OpenAI's GPT-2, a natural language processing\n"
     ]
    }
   ],
   "source": [
    "print(oa.complete('chatGPT is a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T20:50:01.700665Z",
     "start_time": "2023-04-22T20:49:45.629198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are 5 useful prompt templates that can be used in a chatGPT session:\n",
      "\n",
      "1. Can you provide some more details about [topic]?\n",
      "- Examples: Can you provide some more details about the symptoms you're experiencing? Or Can you provide some more details about the issue you're facing with the website?\n",
      "\n",
      "2. How long have you been experiencing [issue]?\n",
      "- Examples: How long have you been experiencing the trouble with your internet connection? Or How long have you been experiencing the pain in your back?\n",
      "\n",
      "3. Have you tried any solutions to resolve [issue]?\n",
      "- Examples: Have you tried any solutions to resolve the error message you're seeing? Or Have you tried any solutions to resolve the trouble you're having with the application?\n",
      "\n",
      "4. What is the specific error message you are receiving?\n",
      "- Examples: What is the specific error message you are receiving when you try to log in? Or What is the specific error message you are receiving when you try to submit the form?\n",
      "\n",
      "5. Is there anything else you would like to add that might be helpful for me to know?\n",
      "- Examples: Is there anything else you would like to add that might be helpful for me to know about your situation? Or Is there anything else you would like to add that might be helpful for me to know about the product you are using?\n"
     ]
    }
   ],
   "source": [
    "print(oa.chat('Act as a chatGPT expert. List 5 useful prompt templates'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T23:17:11.198196Z",
     "start_time": "2023-04-22T23:17:11.175394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-pNE6fCWGN3eJGj7ycFwZREhi.png?st=2023-04-22T22%3A17%3A03Z&se=2023-04-23T00%3A17%3A03Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-22T21%3A08%3A14Z&ske=2023-04-23T21%3A08%3A14Z&sks=b&skv=2021-08-06&sig=5j6LPVO992R95dllAAjbmOXzS0MORD06Fo8unwtGNl0%3D\n"
     ]
    }
   ],
   "source": [
    "url = oa.dalle('An image of Davinci, pop art style')\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T23:51:26.000794Z",
     "start_time": "2023-04-22T23:51:25.977214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-pNE6fCWGN3eJGj7ycFwZREhi.png?st=2023-04-22T22%3A17%3A03Z&se=2023-04-23T00%3A17%3A03Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-22T21%3A08%3A14Z&ske=2023-04-23T21%3A08%3A14Z&sks=b&skv=2021-08-06&sig=5j6LPVO992R95dllAAjbmOXzS0MORD06Fo8unwtGNl0%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `raw` object is a thin layer on top of the `openai` package, which is itself a thin layer over the web requests. \n",
    "\n",
    "What was unsatisfactory with the `openai` package is (1) finding the right function, (2) the signature of the function once you found it, and (3) the documentation of the function. \n",
    "What raw contains is pointers to the main functionalities (not all available -- yet), with nice signatures and documentation, extracted from the web service openAPI specs themselves. \n",
    "\n",
    "For example, to ask chatGPT something, the openai function is `openai.ChatCompletion.create`, or to get simple completions, the function is `openai.Completion.create` whose help is:\n",
    "\n",
    "```\n",
    "Help on method create in module openai.api_resources.completion:\n",
    "\n",
    "create(*args, **kwargs) method of builtins.type instance\n",
    "    Creates a new completion for the provided prompt and parameters.\n",
    "    \n",
    "    See https://platform.openai.com/docs/api-reference/completions/create for a list\n",
    "    of valid parameters.\n",
    "```\n",
    "\n",
    "Not super helpful. It basically tells you to got read the docs elsewhere. \n",
    "\n",
    "The corresponding `raw` function is `raw.completion`, and it's help is a bit more like what you'd expect in a python function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T23:07:26.413989Z",
     "start_time": "2023-04-22T23:07:26.407191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Wrap in module openai.api_resources.chat_completion:\n",
      "\n",
      "chatcompletion\n",
      "    Creates a new chat completion for the provided messages and parameters.\n",
      "    \n",
      "            See https://platform.openai.com/docs/api-reference/chat-completions/create\n",
      "            for a list of valid parameters.\n",
      "    \n",
      "    chatcompletion(\n",
      "            model: str\n",
      "            messages: List[oa.openai_specs.Message]\n",
      "            *\n",
      "            temperature: float = 1\n",
      "            top_p: float = 1\n",
      "            n: int = 1\n",
      "            stream: bool = False\n",
      "            stop=None\n",
      "            max_tokens: int = None\n",
      "            presence_penalty: float = 0\n",
      "            frequency_penalty: float = 0\n",
      "            logit_bias: dict = None\n",
      "            user: str = None\n",
      "    )\n",
      "    \n",
      "    :param model: ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.\n",
      "    \n",
      "    :param messages: The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction).\n",
      "    \n",
      "    :param temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.\n",
      "    \n",
      "    :param top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.\n",
      "    \n",
      "    :param n: How many chat completion choices to generate for each input message.\n",
      "    \n",
      "    :param stream: If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.\n",
      "    \n",
      "    :param stop: Up to 4 sequences where the API will stop generating further tokens.\n",
      "    \n",
      "    :param max_tokens: The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).\n",
      "    \n",
      "    :param presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n",
      "    \n",
      "    :param frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n",
      "    \n",
      "    :param logit_bias: Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n",
      "    \n",
      "    :param user: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(oa.raw.chatcompletion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T23:55:24.856024Z",
     "start_time": "2023-04-22T23:55:13.444809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-78HMPgn3oy2fuvm6sLCgOsQvnTVYr at 0x11fd467a0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"Sure, here are 5 top prompt engineering tricks to write good prompts for chatGPT:\\n\\n1. Be Specific: Ensure that your prompts are specific and clear. The more specific your prompt, the better the response from chatGPT. Avoid using vague or ambiguous language.\\n\\n2. Use Open-Ended Questions: Open-ended questions encourage chatGPT to provide more detailed and personalized responses. Avoid using closed-ended questions that can be answered with a simple yes or no.\\n\\n3. Include Context: Providing context to your prompts helps chatGPT to better understand the topic and provide more relevant responses. Include any necessary background information or details to help guide chatGPT's response.\\n\\n4. Use Emotion: Including emotion in your prompts can help chatGPT generate more engaging and relatable responses. Consider using prompts that evoke emotions such as happiness, sadness, or excitement.\\n\\n5. Test and Refine: Experiment with different prompts and evaluate the responses from chatGPT. Refine your prompts based on the quality of the responses and continue to test and improve over time.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682207713,\n",
       "  \"id\": \"chatcmpl-78HMPgn3oy2fuvm6sLCgOsQvnTVYr\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 214,\n",
       "    \"prompt_tokens\": 36,\n",
       "    \"total_tokens\": 250\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'List 5 top prompt engineering tricks to write good prompts for chatGPT'\n",
    "\n",
    "resp = oa.raw.chatcompletion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at chatGPT\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    model='gpt-3.5-turbo-0301',\n",
    "    temperature=0.5,\n",
    "    max_tokens=300\n",
    ")\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T23:55:52.588053Z",
     "start_time": "2023-04-22T23:55:52.563098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are 5 top prompt engineering tricks to write good prompts for chatGPT:\n",
      "\n",
      "1. Be Specific: Ensure that your prompts are specific and clear. The more specific your prompt, the better the response from chatGPT. Avoid using vague or ambiguous language.\n",
      "\n",
      "2. Use Open-Ended Questions: Open-ended questions encourage chatGPT to provide more detailed and personalized responses. Avoid using closed-ended questions that can be answered with a simple yes or no.\n",
      "\n",
      "3. Include Context: Providing context to your prompts helps chatGPT to better understand the topic and provide more relevant responses. Include any necessary background information or details to help guide chatGPT's response.\n",
      "\n",
      "4. Use Emotion: Including emotion in your prompts can help chatGPT generate more engaging and relatable responses. Consider using prompts that evoke emotions such as happiness, sadness, or excitement.\n",
      "\n",
      "5. Test and Refine: Experiment with different prompts and evaluate the responses from chatGPT. Refine your prompts based on the quality of the responses and continue to test and improve over time.\n"
     ]
    }
   ],
   "source": [
    "print(resp['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-28T06:35:54.175221Z",
     "start_time": "2023-03-28T06:35:54.161355Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAPI OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T20:25:43.275077Z",
     "start_time": "2023-04-22T20:25:43.222964Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T13:43:57.223364Z",
     "start_time": "2023-04-22T13:43:57.204007Z"
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import io\n",
    "import pandas as pd\n",
    "from oa.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:31:49.026355Z",
     "start_time": "2023-04-22T17:31:49.000958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<functools._lru_cache_wrapper at 0x140dcdfe0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align OpenAPI specs and openai python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T20:43:05.105472Z",
     "start_time": "2023-04-22T20:43:01.684244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openapi', 'info', 'servers', 'tags', 'paths', 'components', 'x-oaiMeta']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import oa\n",
    "from oa.openai_specs import *\n",
    "\n",
    "specs = get_openapi_spec_dict()\n",
    "list(specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T20:43:05.108177Z",
     "start_time": "2023-04-22T20:43:05.106295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['moderation',\n",
       " 'embedding',\n",
       " 'completion',\n",
       " 'edit',\n",
       " 'file',\n",
       " 'image',\n",
       " 'chatcompletion',\n",
       " 'finetune']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T20:43:05.141284Z",
     "start_time": "2023-04-22T20:43:05.108904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).\\n',\n",
       " 'default': 'inf',\n",
       " 'type': 'integer'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specs['components']['schemas']['CreateChatCompletionRequest']['properties']['max_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T20:46:35.601812Z",
     "start_time": "2023-04-22T20:46:34.914382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-78EPr1zTxjdDLz5NWdVqVmuCFdSGp at 0x128af5260> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" famous Italian scientist and inventor who lived during the Italian Renaissance period. He was the\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682196395,\n",
       "  \"id\": \"cmpl-78EPr1zTxjdDLz5NWdVqVmuCFdSGp\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = raw.completion(model='text-davinci-003', prompt='Davinci was a')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T20:47:05.057285Z",
     "start_time": "2023-04-22T20:47:05.048578Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T20:43:19.511145Z",
     "start_time": "2023-04-22T20:43:05.148760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-78EMTRRWSB3uGnVV5aJWyx3qqkPVu at 0x11fccbab0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"1. Simplify complex systems: Facades provide a simplified interface to complex systems, reducing the cognitive load on programmers and minimizing the risk of errors.\\n\\n2. Promote decoupling: Facades can serve as a decoupling layer between subsystems, allowing them to vary independently without affecting one another.\\n\\n3. Easy maintenance and refactoring: By hiding the implementation details, facades make it easier to maintain, modify, and refactor code without affecting the clients or other subsystems.\\n\\n4. Enhance security: Facades can encapsulate sensitive functionality and protect it from unauthorized access, thus enhancing security.\\n\\n5. Promote scalability: Facades can enable the addition of new subsystems easily, promoting scalability and adaptability.\\n\\n6. Standardize interfaces: Facades can establish standard interfaces for interacting with subsystems, promoting consistency and reducing errors.\\n\\n7. Improve code organization: Facades help organize code by grouping related functionality into cohesive subsystems. \\n\\n8. Facilitate testing: With facades, testing is simplified since testing a subsystem is limited to the interactions authorized by its facade interface.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682196185,\n",
       "  \"id\": \"chatcmpl-78EMTRRWSB3uGnVV5aJWyx3qqkPVu\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 220,\n",
       "    \"prompt_tokens\": 19,\n",
       "    \"total_tokens\": 239\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = raw.chatcompletion(\n",
    "    model=oa.base.DFLT_MODEL, \n",
    "    messages=[{'role': 'user', 'content': 'What are the advantages of using facades in software programming'}],\n",
    ")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T19:08:02.554951Z",
     "start_time": "2023-04-22T19:07:57.639127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x15f1d9c10> JSON: {\n",
       "  \"created\": 1682190482,\n",
       "  \"data\": [\n",
       "    {\n",
       "      \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-C1cM6Rm5Gt0u0rWsDmMQ0YQi.png?st=2023-04-22T18%3A08%3A02Z&se=2023-04-22T20%3A08%3A02Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-22T16%3A37%3A32Z&ske=2023-04-23T16%3A37%3A32Z&sks=b&skv=2021-08-06&sig=AoUuq0ityXULl%2BQ6nfpyXIrtvPFRxskIqSjDI5qAVfc%3D\"\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oa.dalle('Simple interface to a complex system', user='oa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T19:08:27.257037Z",
     "start_time": "2023-04-22T19:08:27.233365Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Image.create of <class 'openai.api_resources.image.Image'>>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oa.openai.Image.create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T19:33:10.674650Z",
     "start_time": "2023-04-22T19:33:05.525999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x12f9d6ed0> JSON: {\n",
       "  \"created\": 1682191990,\n",
       "  \"data\": [\n",
       "    {\n",
       "      \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-MgRw6okqv6hSRjGPSymBHKDj.png?st=2023-04-22T18%3A33%3A10Z&se=2023-04-22T20%3A33%3A10Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-22T14%3A54%3A20Z&ske=2023-04-23T14%3A54%3A20Z&sks=b&skv=2021-08-06&sig=cDVJkyhgshlFElvEHSrp7WC26YmZ7lmSA8RR%2BKTSgDA%3D\"\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp2 = t.image('Simple interface to a complex system', size='512x512')\n",
    "resp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrap to make raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T19:32:03.298669Z",
     "start_time": "2023-04-22T19:32:03.286058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sig (prompt: str, *, n: int = 1, size: str = '1024x1024', response_format: str = 'url', user: str = 'oa')>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.CreateImageRequest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T19:31:01.587547Z",
     "start_time": "2023-04-22T19:30:55.396468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x12fba1580> JSON: {\n",
       "  \"created\": 1682191861,\n",
       "  \"data\": [\n",
       "    {\n",
       "      \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-iKnxgqk3bsBvyqpTwqACgQ3q.png?st=2023-04-22T18%3A31%3A01Z&se=2023-04-22T20%3A31%3A01Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-04-22T15%3A28%3A53Z&ske=2023-04-23T15%3A28%3A53Z&sks=b&skv=2021-08-06&sig=%2BsWS0gGbxv9FcXRg5O1XZe/LWo4IsLthw/eAqCN0p3k%3D\"\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.CreateImageRequest\n",
    "\n",
    "schemas['CreateImageRequest']\n",
    "oa.openai.Image.create(prompt='Davinci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T19:12:04.508110Z",
     "start_time": "2023-04-22T19:12:04.477639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sig (*args, **kwargs)>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import i2\n",
    "i2.Sig(oa.openai.Image.create)\n",
    "i2.Sig(oa.openai.Completion.create)\n",
    "i2.Sig(oa.openai.ChatCompletion.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T19:13:20.478534Z",
     "start_time": "2023-04-22T19:13:20.456290Z"
    }
   },
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T19:14:54.019543Z",
     "start_time": "2023-04-22T19:14:53.968606Z"
    }
   },
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Must provide an 'engine' or 'model' parameter to create a <class 'openai.api_resources.completion.Completion'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[232], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m w \u001b[38;5;241m=\u001b[39m oa\u001b[38;5;241m.\u001b[39mopenai\u001b[38;5;241m.\u001b[39mCompletion(model\u001b[38;5;241m=\u001b[39moa\u001b[38;5;241m.\u001b[39mbase\u001b[38;5;241m.\u001b[39mDFLT_ENGINE, prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDavinci was a\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[1;32m    141\u001b[0m         timeout,\n\u001b[1;32m    142\u001b[0m         stream,\n\u001b[1;32m    143\u001b[0m         headers,\n\u001b[1;32m    144\u001b[0m         request_timeout,\n\u001b[1;32m    145\u001b[0m         typed_api_type,\n\u001b[1;32m    146\u001b[0m         requestor,\n\u001b[1;32m    147\u001b[0m         url,\n\u001b[1;32m    148\u001b[0m         params,\n\u001b[0;32m--> 149\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__prepare_create_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m requestor\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:90\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[0;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mInvalidRequestError(\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust provide an \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter to create a \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m         )\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# No special timeout handling\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: Must provide an 'engine' or 'model' parameter to create a <class 'openai.api_resources.completion.Completion'>"
     ]
    }
   ],
   "source": [
    "w = oa.openai.Completion(model=oa.base.DFLT_ENGINE, prompt='Davinci was a')\n",
    "w.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T19:14:34.701725Z",
     "start_time": "2023-04-22T19:14:33.416016Z"
    }
   },
   "outputs": [],
   "source": [
    "from i2 import wrap\n",
    "\n",
    "def gen():\n",
    "    for name in matched_names:\n",
    "        sig_for_func = getattr(sig, f\"Create{name}Request\")\n",
    "        func = wrap(getattr(oa.openai, name).create, name = name.lower())\n",
    "        yield name.lower(), sig_for_func(func)\n",
    "        \n",
    "        \n",
    "t = list(gen())[2][1]\n",
    "\n",
    "resp = t(model='text-davinci-003', prompt='Davinci was a')\n",
    "# <OpenAIObject text_completion id=cmpl-78CeaQkdM5aZxaTaYiEWLzP2kaMWp at 0x15eb07d80> JSON: {\n",
    "#   \"choices\": [\n",
    "#     {\n",
    "#       \"finish_reason\": \"length\",\n",
    "#       \"index\": 0,\n",
    "#       \"logprobs\": null,\n",
    "#       \"text\": \" scientist.\\n\\nLeonardo da Vinci (1452-1519)\"\n",
    "#     }\n",
    "#   ],\n",
    "#   \"created\": 1682189620,\n",
    "#   \"id\": \"cmpl-78CeaQkdM5aZxaTaYiEWLzP2kaMWp\",\n",
    "#   \"model\": \"text-davinci-003\",\n",
    "#   \"object\": \"text_completion\",\n",
    "#   \"usage\": {\n",
    "#     \"completion_tokens\": 16,\n",
    "#     \"prompt_tokens\": 5,\n",
    "#     \"total_tokens\": 21\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T18:55:35.241882Z",
     "start_time": "2023-04-22T18:55:35.210818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'cmpl-78CeaQkdM5aZxaTaYiEWLzP2kaMWp',\n",
       " 'object': 'text_completion',\n",
       " 'created': 1682189620,\n",
       " 'model': 'text-davinci-003',\n",
       " 'choices': [{'text': ' scientist.\\n\\nLeonardo da Vinci (1452-1519)',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'length'}],\n",
       " 'usage': {'prompt_tokens': 5, 'completion_tokens': 16, 'total_tokens': 21}}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.to_dict_recursive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T18:56:10.929776Z",
     "start_time": "2023-04-22T18:56:10.901514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' scientist.\\n\\nLeonardo da Vinci (1452-1519)'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T18:52:52.239566Z",
     "start_time": "2023-04-22T18:52:52.213322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text-davinci-003'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oa.base.DFLT_ENGINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T18:42:05.498399Z",
     "start_time": "2023-04-22T18:42:04.723692Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<i2.Wrap asd(x)>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrap(lambda x: x, name='asd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T18:27:13.953772Z",
     "start_time": "2023-04-22T18:27:13.927384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sig (input, *, model: str = 'text-moderation-latest')>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.CreateTranscriptionRequest\n",
    "oa.openai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T18:24:02.245755Z",
     "start_time": "2023-04-22T18:24:02.220639Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Image',\n",
       " 'ContextVar',\n",
       " 'OpenAIError',\n",
       " 'Deployment',\n",
       " 'Engine',\n",
       " 'ErrorObject',\n",
       " 'Edit',\n",
       " 'Model',\n",
       " 'Completion',\n",
       " 'Embedding',\n",
       " 'Moderation',\n",
       " 'FineTune',\n",
       " 'Customer',\n",
       " 'File',\n",
       " 'APIError',\n",
       " 'InvalidRequestError',\n",
       " 'ChatCompletion',\n",
       " 'Audio']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in list(attrs) if isinstance(getattr(oa.openai, x), type)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specs/shema scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T13:46:14.690090Z",
     "start_time": "2023-03-29T13:46:14.661352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import re\n",
    "from dol.paths import _path_get as path_get  # intending on switching both!\n",
    "\n",
    "from dol.recipes import search_paths\n",
    "\n",
    "def string_matching(pattern, p, k, v):\n",
    "    matches_string = re.compile(pattern).search\n",
    "    return (\n",
    "        isinstance(k, str) and matches_string(k)\n",
    "        or\n",
    "        isinstance(v, str) and matches_string(v)\n",
    "    )\n",
    "\n",
    "it = search_paths(specs, pkv_filt=partial(string_matching, '\\$ref'))\n",
    "t = list(it)\n",
    "print(f'{len(t)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T14:25:42.936670Z",
     "start_time": "2023-04-22T14:25:42.910732Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T13:46:15.789126Z",
     "start_time": "2023-03-29T13:46:15.751898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jsonref\n",
    "expanded_specs = jsonref.JsonRef.replace_refs(specs)\n",
    "list(search_paths(expanded_specs, pkv_filt=partial(string_matching, '\\$ref')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T13:46:24.344107Z",
     "start_time": "2023-03-29T13:46:24.327927Z"
    }
   },
   "outputs": [],
   "source": [
    "from http2py import HttpClient\n",
    "api = HttpClient(openapi_spec=expanded_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T13:46:26.490566Z",
     "start_time": "2023-03-29T13:46:26.478465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auth_type',\n",
       " 'base_url',\n",
       " 'ensure_login',\n",
       " 'handle_request',\n",
       " 'init_security',\n",
       " 'is_request_method',\n",
       " 'login_input_keys',\n",
       " 'login_url',\n",
       " 'openapi_spec',\n",
       " 'receive_login',\n",
       " 'refresh_input_keys',\n",
       " 'refresh_inputs',\n",
       " 'refresh_login',\n",
       " 'refresh_url',\n",
       " 'register_method',\n",
       " 'request_func',\n",
       " 'session',\n",
       " 'set_header',\n",
       " 'set_profile',\n",
       " 'title',\n",
       " 'verify_cert',\n",
       " 'version']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x for x in dir(api) if not x.startswith('_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T12:46:55.447435Z",
     "start_time": "2023-03-29T12:46:55.416199Z"
    }
   },
   "outputs": [],
   "source": [
    "# from openapi_python_client import generate_client\n",
    "\n",
    "# OpenAPIGenerator.from_url(\n",
    "#     'https://raw.githubusercontent.com/openai/openai-openapi/master/openapi.yaml',\n",
    "#     output_dir='example_api_client',\n",
    "#     package_name='example_api_client'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T12:46:49.068869Z",
     "start_time": "2023-03-29T12:46:49.044869Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add the 'my_project' folder to the Python path\n",
    "sys.path.insert(0, '/Users/thorwhalen/tmp/open-ai-api-client')\n",
    "\n",
    "# # Import a module from the 'my_package' package\n",
    "# from my_package import my_module\n",
    "\n",
    "import open_ai_api_client as oc\n",
    "import open_ai_api_client.api\n",
    "from open_ai_api_client.api import open_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import extrude\n",
    "# from extrude import mk_api, run_api\n",
    "\n",
    "# def _make_children_story(story, image_style):\n",
    "#     return make_children_story(story, image_style)\n",
    "\n",
    "# app = mk_api([_make_children_story])\n",
    "# run_api(app, server='wsgiref')\n",
    "\n",
    "# check out:\n",
    "\n",
    "# http://localhost:3030/openapi\n",
    "\n",
    "# http://localhost:3030/swagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T16:51:27.793298Z",
     "start_time": "2023-04-22T16:51:27.753409Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T16:52:24.212594Z",
     "start_time": "2023-04-22T16:52:24.190991Z"
    }
   },
   "outputs": [],
   "source": [
    "from oa.util import *\n",
    "from oa.openai_specs import *\n",
    "# import oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T16:59:27.831896Z",
     "start_time": "2023-04-22T16:59:26.589925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-78As2dcVt7dtn4oaOhnGv7eEatKbM at 0x1570724d0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"This is a dedicated private space exclusively only for you. It is the best option\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682182766,\n",
       "  \"id\": \"cmpl-78As2dcVt7dtn4oaOhnGv7eEatKbM\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 1,\n",
       "    \"total_tokens\": 17\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from i2 import wrap\n",
    "\n",
    "f = wrap(oa.openai.Completion.create)\n",
    "f.__signature__ = sig.CreateCompletionRequest\n",
    "f(model=oa.base.DFLT_ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:04:32.495235Z",
     "start_time": "2023-04-22T17:04:32.468802Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Answer',\n",
       " 'Classification',\n",
       " 'ImageEdit',\n",
       " 'ImageVariation',\n",
       " 'Search',\n",
       " 'Transcription',\n",
       " 'Translation'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [x[len('Create'):-len('Request')] for x in vars(sig) if x.startswith('Create') and x.endswith('Request')]\n",
    "aligned_service_names = [aa for aa in a if hasattr(oa.openai, aa)]\n",
    "not_aligned_service_names = set(a) - set(aligned_service_names)\n",
    "not_aligned_service_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:07:05.867138Z",
     "start_time": "2023-04-22T17:07:05.843854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sig (file: str, model: str, *, prompt: str, language: str, response_format: str = 'json', temperature: float = 0)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oa.openai.Audio.transcribe\n",
    "\n",
    "sig.CreateTranscriptionRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:07:50.424499Z",
     "start_time": "2023-04-22T17:07:50.397006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openapi', 'info', 'servers', 'tags', 'paths', 'components', 'x-oaiMeta']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:08:44.111907Z",
     "start_time": "2023-04-22T17:08:44.089350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/engines',\n",
       " '/engines/{engine_id}',\n",
       " '/completions',\n",
       " '/chat/completions',\n",
       " '/edits',\n",
       " '/images/generations',\n",
       " '/images/edits',\n",
       " '/images/variations',\n",
       " '/embeddings',\n",
       " '/audio/transcriptions',\n",
       " '/audio/translations',\n",
       " '/engines/{engine_id}/search',\n",
       " '/files',\n",
       " '/files/{file_id}',\n",
       " '/files/{file_id}/content',\n",
       " '/answers',\n",
       " '/classifications',\n",
       " '/fine-tunes',\n",
       " '/fine-tunes/{fine_tune_id}',\n",
       " '/fine-tunes/{fine_tune_id}/cancel',\n",
       " '/fine-tunes/{fine_tune_id}/events',\n",
       " '/models',\n",
       " '/models/{model}',\n",
       " '/moderations']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = specs['paths']\n",
    "list(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:09:41.831201Z",
     "start_time": "2023-04-22T17:09:41.804652Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['operationId', 'tags', 'summary', 'requestBody', 'responses', 'x-oaiMeta']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(paths['/chat/completions']['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:09:51.307531Z",
     "start_time": "2023-04-22T17:09:51.283392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operationId': 'createChatCompletion',\n",
       " 'tags': ['OpenAI'],\n",
       " 'summary': 'Creates a completion for the chat message',\n",
       " 'requestBody': {'required': True,\n",
       "  'content': {'application/json': {'schema': {'type': 'object',\n",
       "     'properties': {'model': {'description': 'ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.',\n",
       "       'type': 'string'},\n",
       "      'messages': {'description': 'The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction).',\n",
       "       'type': 'array',\n",
       "       'minItems': 1,\n",
       "       'items': {'type': 'object',\n",
       "        'properties': {'role': {'type': 'string',\n",
       "          'enum': ['system', 'user', 'assistant'],\n",
       "          'description': 'The role of the author of this message.'},\n",
       "         'content': {'type': 'string',\n",
       "          'description': 'The contents of the message'},\n",
       "         'name': {'type': 'string',\n",
       "          'description': 'The name of the user in a multi-user chat'}},\n",
       "        'required': ['role', 'content']}},\n",
       "      'temperature': {'type': 'number',\n",
       "       'minimum': 0,\n",
       "       'maximum': 2,\n",
       "       'default': 1,\n",
       "       'example': 1,\n",
       "       'nullable': True,\n",
       "       'description': 'What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\\n\\nWe generally recommend altering this or `top_p` but not both.\\n'},\n",
       "      'top_p': {'type': 'number',\n",
       "       'minimum': 0,\n",
       "       'maximum': 1,\n",
       "       'default': 1,\n",
       "       'example': 1,\n",
       "       'nullable': True,\n",
       "       'description': 'An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\\n\\nWe generally recommend altering this or `temperature` but not both.\\n'},\n",
       "      'n': {'type': 'integer',\n",
       "       'minimum': 1,\n",
       "       'maximum': 128,\n",
       "       'default': 1,\n",
       "       'example': 1,\n",
       "       'nullable': True,\n",
       "       'description': 'How many chat completion choices to generate for each input message.'},\n",
       "      'stream': {'description': 'If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.\\n',\n",
       "       'type': 'boolean',\n",
       "       'nullable': True,\n",
       "       'default': False},\n",
       "      'stop': {'description': 'Up to 4 sequences where the API will stop generating further tokens.\\n',\n",
       "       'default': None,\n",
       "       'oneOf': [{'type': 'string', 'nullable': True},\n",
       "        {'type': 'array',\n",
       "         'minItems': 1,\n",
       "         'maxItems': 4,\n",
       "         'items': {'type': 'string'}}]},\n",
       "      'max_tokens': {'description': 'The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).\\n',\n",
       "       'default': 'inf',\n",
       "       'type': 'integer'},\n",
       "      'presence_penalty': {'type': 'number',\n",
       "       'default': 0,\n",
       "       'minimum': -2,\n",
       "       'maximum': 2,\n",
       "       'nullable': True,\n",
       "       'description': \"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\\n\\n[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\\n\"},\n",
       "      'frequency_penalty': {'type': 'number',\n",
       "       'default': 0,\n",
       "       'minimum': -2,\n",
       "       'maximum': 2,\n",
       "       'nullable': True,\n",
       "       'description': \"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\\n\\n[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\\n\"},\n",
       "      'logit_bias': {'type': 'object',\n",
       "       'x-oaiTypeLabel': 'map',\n",
       "       'default': None,\n",
       "       'nullable': True,\n",
       "       'description': 'Modify the likelihood of specified tokens appearing in the completion.\\n\\nAccepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\\n'},\n",
       "      'user': {'type': 'string',\n",
       "       'example': 'user-1234',\n",
       "       'description': 'A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\\n'}},\n",
       "     'required': ['model', 'messages']}}}},\n",
       " 'responses': {'200': {'description': 'OK',\n",
       "   'content': {'application/json': {'schema': {'type': 'object',\n",
       "      'properties': {'id': {'type': 'string'},\n",
       "       'object': {'type': 'string'},\n",
       "       'created': {'type': 'integer'},\n",
       "       'model': {'type': 'string'},\n",
       "       'choices': {'type': 'array',\n",
       "        'items': {'type': 'object',\n",
       "         'properties': {'index': {'type': 'integer'},\n",
       "          'message': {'type': 'object',\n",
       "           'properties': {'role': {'type': 'string',\n",
       "             'enum': ['system', 'user', 'assistant'],\n",
       "             'description': 'The role of the author of this message.'},\n",
       "            'content': {'type': 'string',\n",
       "             'description': 'The contents of the message'}},\n",
       "           'required': ['role', 'content']},\n",
       "          'finish_reason': {'type': 'string'}}}},\n",
       "       'usage': {'type': 'object',\n",
       "        'properties': {'prompt_tokens': {'type': 'integer'},\n",
       "         'completion_tokens': {'type': 'integer'},\n",
       "         'total_tokens': {'type': 'integer'}},\n",
       "        'required': ['prompt_tokens', 'completion_tokens', 'total_tokens']}},\n",
       "      'required': ['id', 'object', 'created', 'model', 'choices']}}}}},\n",
       " 'x-oaiMeta': {'name': 'Create chat completion',\n",
       "  'group': 'chat',\n",
       "  'path': 'create',\n",
       "  'beta': True,\n",
       "  'examples': {'curl': 'curl https://api.openai.com/v1/chat/completions \\\\\\n  -H \\'Content-Type: application/json\\' \\\\\\n  -H \\'Authorization: Bearer YOUR_API_KEY\\' \\\\\\n  -d \\'{\\n  \"model\": \"gpt-3.5-turbo\",\\n  \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\\n}\\'\\n',\n",
       "   'python': 'import os\\nimport openai\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\n\\ncompletion = openai.ChatCompletion.create(\\n  model=\"gpt-3.5-turbo\",\\n  messages=[\\n    {\"role\": \"user\", \"content\": \"Hello!\"}\\n  ]\\n)\\n\\nprint(completion.choices[0].message)\\n',\n",
       "   'node.js': 'const { Configuration, OpenAIApi } = require(\"openai\");\\n\\nconst configuration = new Configuration({\\n  apiKey: process.env.OPENAI_API_KEY,\\n});\\nconst openai = new OpenAIApi(configuration);\\n\\nconst completion = await openai.createChatCompletion({\\n  model: \"gpt-3.5-turbo\",\\n  messages: [{role: \"user\", content: \"Hello world\"}],\\n});\\nconsole.log(completion.data.choices[0].message);\\n'},\n",
       "  'parameters': '{\\n  \"model\": \"gpt-3.5-turbo\",\\n  \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\\n}\\n',\n",
       "  'response': '{\\n  \"id\": \"chatcmpl-123\",\\n  \"object\": \"chat.completion\",\\n  \"created\": 1677652288,\\n  \"choices\": [{\\n    \"index\": 0,\\n    \"message\": {\\n      \"role\": \"assistant\",\\n      \"content\": \"\\\\n\\\\nHello there, how may I assist you today?\",\\n    },\\n    \"finish_reason\": \"stop\"\\n  }],\\n  \"usage\": {\\n    \"prompt_tokens\": 9,\\n    \"completion_tokens\": 12,\\n    \"total_tokens\": 21\\n  }\\n}\\n'}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths['/chat/completions']['post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:16:57.746333Z",
     "start_time": "2023-04-22T17:16:57.714226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paths', '/engines', 'get', 'operationId'),\n",
       " ('paths', '/engines/{engine_id}', 'get', 'operationId'),\n",
       " ('paths', '/completions', 'post', 'operationId'),\n",
       " ('paths', '/chat/completions', 'post', 'operationId'),\n",
       " ('paths', '/edits', 'post', 'operationId'),\n",
       " ('paths', '/images/generations', 'post', 'operationId'),\n",
       " ('paths', '/images/edits', 'post', 'operationId'),\n",
       " ('paths', '/images/variations', 'post', 'operationId'),\n",
       " ('paths', '/embeddings', 'post', 'operationId'),\n",
       " ('paths', '/audio/transcriptions', 'post', 'operationId'),\n",
       " ('paths', '/audio/translations', 'post', 'operationId'),\n",
       " ('paths', '/engines/{engine_id}/search', 'post', 'operationId'),\n",
       " ('paths', '/files', 'get', 'operationId'),\n",
       " ('paths', '/files', 'post', 'operationId'),\n",
       " ('paths', '/files/{file_id}', 'delete', 'operationId'),\n",
       " ('paths', '/files/{file_id}', 'get', 'operationId'),\n",
       " ('paths', '/files/{file_id}/content', 'get', 'operationId'),\n",
       " ('paths', '/answers', 'post', 'operationId'),\n",
       " ('paths', '/classifications', 'post', 'operationId'),\n",
       " ('paths', '/fine-tunes', 'post', 'operationId'),\n",
       " ('paths', '/fine-tunes', 'get', 'operationId'),\n",
       " ('paths', '/fine-tunes/{fine_tune_id}', 'get', 'operationId'),\n",
       " ('paths', '/fine-tunes/{fine_tune_id}/cancel', 'post', 'operationId'),\n",
       " ('paths', '/fine-tunes/{fine_tune_id}/events', 'get', 'operationId'),\n",
       " ('paths', '/models', 'get', 'operationId'),\n",
       " ('paths', '/models/{model}', 'get', 'operationId'),\n",
       " ('paths', '/models/{model}', 'delete', 'operationId'),\n",
       " ('paths', '/moderations', 'post', 'operationId')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:10:25.015054Z",
     "start_time": "2023-04-22T17:10:24.990771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Creates a completion for the chat message'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths['/chat/completions']['post']['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa.openai.ChatCompletion.create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:12:16.578213Z",
     "start_time": "2023-04-22T17:12:16.548739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'object',\n",
       " 'properties': {'model': {'description': 'ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.',\n",
       "   'type': 'string'},\n",
       "  'messages': {'description': 'The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction).',\n",
       "   'type': 'array',\n",
       "   'minItems': 1,\n",
       "   'items': {'type': 'object',\n",
       "    'properties': {'role': {'type': 'string',\n",
       "      'enum': ['system', 'user', 'assistant'],\n",
       "      'description': 'The role of the author of this message.'},\n",
       "     'content': {'type': 'string',\n",
       "      'description': 'The contents of the message'},\n",
       "     'name': {'type': 'string',\n",
       "      'description': 'The name of the user in a multi-user chat'}},\n",
       "    'required': ['role', 'content']}},\n",
       "  'temperature': {'type': 'number',\n",
       "   'minimum': 0,\n",
       "   'maximum': 2,\n",
       "   'default': 1,\n",
       "   'example': 1,\n",
       "   'nullable': True,\n",
       "   'description': 'What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\\n\\nWe generally recommend altering this or `top_p` but not both.\\n'},\n",
       "  'top_p': {'type': 'number',\n",
       "   'minimum': 0,\n",
       "   'maximum': 1,\n",
       "   'default': 1,\n",
       "   'example': 1,\n",
       "   'nullable': True,\n",
       "   'description': 'An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\\n\\nWe generally recommend altering this or `temperature` but not both.\\n'},\n",
       "  'n': {'type': 'integer',\n",
       "   'minimum': 1,\n",
       "   'maximum': 128,\n",
       "   'default': 1,\n",
       "   'example': 1,\n",
       "   'nullable': True,\n",
       "   'description': 'How many chat completion choices to generate for each input message.'},\n",
       "  'stream': {'description': 'If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.\\n',\n",
       "   'type': 'boolean',\n",
       "   'nullable': True,\n",
       "   'default': False},\n",
       "  'stop': {'description': 'Up to 4 sequences where the API will stop generating further tokens.\\n',\n",
       "   'default': None,\n",
       "   'oneOf': [{'type': 'string', 'nullable': True},\n",
       "    {'type': 'array',\n",
       "     'minItems': 1,\n",
       "     'maxItems': 4,\n",
       "     'items': {'type': 'string'}}]},\n",
       "  'max_tokens': {'description': 'The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).\\n',\n",
       "   'default': 'inf',\n",
       "   'type': 'integer'},\n",
       "  'presence_penalty': {'type': 'number',\n",
       "   'default': 0,\n",
       "   'minimum': -2,\n",
       "   'maximum': 2,\n",
       "   'nullable': True,\n",
       "   'description': \"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\\n\\n[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\\n\"},\n",
       "  'frequency_penalty': {'type': 'number',\n",
       "   'default': 0,\n",
       "   'minimum': -2,\n",
       "   'maximum': 2,\n",
       "   'nullable': True,\n",
       "   'description': \"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\\n\\n[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\\n\"},\n",
       "  'logit_bias': {'type': 'object',\n",
       "   'x-oaiTypeLabel': 'map',\n",
       "   'default': None,\n",
       "   'nullable': True,\n",
       "   'description': 'Modify the likelihood of specified tokens appearing in the completion.\\n\\nAccepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\\n'},\n",
       "  'user': {'type': 'string',\n",
       "   'example': 'user-1234',\n",
       "   'description': 'A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\\n'}},\n",
       " 'required': ['model', 'messages']}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths['/chat/completions']['post']['requestBody']['content']['application/json']['schema']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T17:02:46.879047Z",
     "start_time": "2023-04-22T17:02:46.854549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion\n",
      "ChatCompletion\n",
      "Edit\n",
      "Image\n",
      "Moderation\n",
      "File\n",
      "FineTune\n",
      "Embedding\n"
     ]
    }
   ],
   "source": [
    "for aa in a:\n",
    "    if hasattr(oa.openai, aa):\n",
    "        print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T16:53:29.596200Z",
     "start_time": "2023-04-22T16:53:29.552760Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'method' object has no attribute '__signature__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m i2\u001b[38;5;241m.\u001b[39mSig(oa\u001b[38;5;241m.\u001b[39mopenai\u001b[38;5;241m.\u001b[39mChatCompletion\u001b[38;5;241m.\u001b[39mcreate)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mi2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/py/proj/i/i2/i2/signatures.py:1121\u001b[0m, in \u001b[0;36mSig.__call__\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, func: Callable):\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;124;03m\"\"\"Gives the input function the signature.\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;124;03m    Just calls Sig.wrap so see docs of Sig.wrap (which contains examples and\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;124;03m    doctests).\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/py/proj/i/i2/i2/signatures.py:1101\u001b[0m, in \u001b[0;36mSig.wrap\u001b[0;34m(self, func, ignore_incompatible_signatures, copy_function)\u001b[0m\n\u001b[1;32m   1097\u001b[0m _validate_sanity_of_signature_change(func, \u001b[38;5;28mself\u001b[39m, ignore_incompatible_signatures)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Change (mutate!) func, writing a new __signature__, __annotations__,\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;66;03m# __defaults__ and __kwdefaults__\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__signature__\u001b[49m \u001b[38;5;241m=\u001b[39m Sig(\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues(), return_annotation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_annotation\n\u001b[1;32m   1103\u001b[0m )\n\u001b[1;32m   1104\u001b[0m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__annotations__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations\n\u001b[1;32m   1105\u001b[0m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__defaults__\u001b[39m, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__kwdefaults__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dunder_defaults_and_kwdefaults()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'method' object has no attribute '__signature__'"
     ]
    }
   ],
   "source": [
    "import oa\n",
    "i2.Sig(oa.openai.ChatCompletion.create)\n",
    "\n",
    "i2.Sig('a')(oa.openai.ChatCompletion.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T16:38:49.232779Z",
     "start_time": "2023-04-22T16:38:49.221446Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.',\n",
       " 'type': 'string'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemas['CreateCompletionRequest']['properties']['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T16:38:49.244886Z",
     "start_time": "2023-04-22T16:38:49.234818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sig (model: str, messages: List[oa.openai_specs.Message], *, temperature: float = 1, top_p: float = 1, n: int = 1, stream: bool = False, stop=None, max_tokens: int = 'inf', presence_penalty: float = 0, frequency_penalty: float = 0, logit_bias: dict = None, user: str = 'oa')>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.CreateChatCompletionRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T16:38:49.255582Z",
     "start_time": "2023-04-22T16:38:49.245752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model',\n",
       " 'prompt',\n",
       " 'suffix',\n",
       " 'max_tokens',\n",
       " 'temperature',\n",
       " 'top_p',\n",
       " 'n',\n",
       " 'stream',\n",
       " 'logprobs',\n",
       " 'echo',\n",
       " 'stop',\n",
       " 'presence_penalty',\n",
       " 'frequency_penalty',\n",
       " 'best_of',\n",
       " 'logit_bias',\n",
       " 'user']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.CreateImageRequest.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T16:39:01.981450Z",
     "start_time": "2023-04-22T16:39:01.955946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sig (prompt: str = None, model: str = 'gpt-3.5-turbo', messages: List[oa.openai_specs.Message] = None, *, temperature: float = 1, top_p: float = 1, n: int = 1, stream: bool = False, stop=None, max_tokens: int = 'inf', presence_penalty: float = 0, frequency_penalty: float = 0, logit_bias: dict = None, user: str = 'oa')>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import i2, oa\n",
    "\n",
    "i2.Sig(oa.chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T16:40:26.190872Z",
     "start_time": "2023-04-22T16:40:26.159552Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "chat() missing 1 required keyword-only argument: 'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moa\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43moa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWhat are the pros and cons of layering engineering complexities with a simple interface?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: chat() missing 1 required keyword-only argument: 'messages'"
     ]
    }
   ],
   "source": [
    "import oa\n",
    "\n",
    "resp = oa.chat('What are the pros and cons of layering engineering complexities with a simple interface?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa.openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T16:31:15.959760Z",
     "start_time": "2023-04-22T16:31:15.933962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sig (model: str, messages: List[oa.openai_specs.Message], *, temperature: float = 1, top_p: float = 1, n: int = 1, stream: bool = False, stop=None, max_tokens: int = 'inf', presence_penalty: float = 0, frequency_penalty: float = 0, logit_bias: dict = None, user: str = 'oa')>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig.CreateChatCompletionRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:45:42.258738Z",
     "start_time": "2023-04-22T15:45:42.252737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListEnginesResponse ListModelsResponse DeleteModelResponse CreateCompletionRequest CreateCompletionResponse ChatCompletionRequestMessage ChatCompletionResponseMessage CreateChatCompletionRequest CreateChatCompletionResponse CreateEditRequest CreateEditResponse CreateImageRequest ImagesResponse CreateImageEditRequest CreateImageVariationRequest CreateModerationRequest CreateModerationResponse CreateSearchRequest CreateSearchResponse ListFilesResponse CreateFileRequest DeleteFileResponse CreateAnswerRequest CreateAnswerResponse CreateClassificationRequest CreateClassificationResponse CreateFineTuneRequest ListFineTunesResponse ListFineTuneEventsResponse CreateEmbeddingRequest CreateEmbeddingResponse CreateTranscriptionRequest CreateTranscriptionResponse CreateTranslationRequest CreateTranslationResponse Engine Model OpenAIFile FineTune FineTuneEvent\n"
     ]
    }
   ],
   "source": [
    "schemas = specs['components']['schemas']\n",
    "print(*schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T13:44:05.310907Z",
     "start_time": "2023-04-22T13:44:05.285791Z"
    }
   },
   "outputs": [],
   "source": [
    "# api = HttpClient(openapi_spec=specs)\n",
    "# print(*dir(api))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:44:52.764835Z",
     "start_time": "2023-04-22T15:44:52.747194Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Param \"as=1\">, <Param \"aa\">]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([Param('as', default=1), Param('aa', Param.KO)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:42:36.335232Z",
     "start_time": "2023-04-22T15:42:36.284177Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'user'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m Sig(foo)\u001b[38;5;241m.\u001b[39mkwargs_from_args_and_kwargs(args, kwargs)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(kwargs)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mfoo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 5\u001b[0m, in \u001b[0;36mfoo\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@sig\u001b[39m\u001b[38;5;241m.\u001b[39mCreateChatCompletionRequest\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfoo\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m----> 5\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mSig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfoo\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs_from_args_and_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(kwargs)\n",
      "File \u001b[0;32m~/Dropbox/py/proj/i/i2/i2/signatures.py:2352\u001b[0m, in \u001b[0;36mSig.kwargs_from_args_and_kwargs\u001b[0;34m(self, args, kwargs, apply_defaults, allow_partial, allow_excess, ignore_kind, debug)\u001b[0m\n\u001b[1;32m   2347\u001b[0m     max_allowed_num_of_posisional_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\n\u001b[1;32m   2348\u001b[0m         k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m PK \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkinds\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[1;32m   2349\u001b[0m     )\n\u001b[1;32m   2350\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[:max_allowed_num_of_posisional_args]\n\u001b[0;32m-> 2352\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[43mbinder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msig_relevant_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m apply_defaults:\n\u001b[1;32m   2354\u001b[0m     b\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/inspect.py:3182\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3178\u001b[0m     \u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/inspect.py:3152\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m   3146\u001b[0m     \u001b[38;5;66;03m# We have no value for this parameter.  It's fine though,\u001b[39;00m\n\u001b[1;32m   3147\u001b[0m     \u001b[38;5;66;03m# if it has a default value, or it is an '*args'-like\u001b[39;00m\n\u001b[1;32m   3148\u001b[0m     \u001b[38;5;66;03m# parameter, left alone by the processing of positional\u001b[39;00m\n\u001b[1;32m   3149\u001b[0m     \u001b[38;5;66;03m# arguments.\u001b[39;00m\n\u001b[1;32m   3150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m partial \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m _VAR_POSITIONAL \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m   3151\u001b[0m                                         param\u001b[38;5;241m.\u001b[39mdefault \u001b[38;5;129;01mis\u001b[39;00m _empty):\n\u001b[0;32m-> 3152\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing a required argument: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39m \\\n\u001b[1;32m   3153\u001b[0m                         \u001b[38;5;28mformat\u001b[39m(arg\u001b[38;5;241m=\u001b[39mparam_name)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   3155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m _POSITIONAL_ONLY:\n\u001b[1;32m   3157\u001b[0m         \u001b[38;5;66;03m# This should never happen in case of a properly built\u001b[39;00m\n\u001b[1;32m   3158\u001b[0m         \u001b[38;5;66;03m# Signature object (but let's have this check here\u001b[39;00m\n\u001b[1;32m   3159\u001b[0m         \u001b[38;5;66;03m# to ensure correct behaviour just in case)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: missing a required argument: 'user'"
     ]
    }
   ],
   "source": [
    "from i2 import Sig, Param\n",
    "\n",
    "@sig.CreateChatCompletionRequest\n",
    "def foo(*args, **kwargs):\n",
    "    kwargs = Sig(foo).kwargs_from_args_and_kwargs(args, kwargs)\n",
    "    print(kwargs)\n",
    "\n",
    "foo(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:30:54.025332Z",
     "start_time": "2023-04-22T15:30:54.016978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.',\n",
       " 'type': 'string'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = schemas['CreateChatCompletionRequest']\n",
    "tt['properties']['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:31:11.236781Z",
     "start_time": "2023-04-22T15:31:11.188723Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>minItems</th>\n",
       "      <th>items</th>\n",
       "      <th>minimum</th>\n",
       "      <th>maximum</th>\n",
       "      <th>default</th>\n",
       "      <th>example</th>\n",
       "      <th>nullable</th>\n",
       "      <th>oneOf</th>\n",
       "      <th>x-oaiTypeLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>ID of the model to use. Currently, only `gpt-3...</td>\n",
       "      <td>string</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>messages</th>\n",
       "      <td>The messages to generate chat completions for,...</td>\n",
       "      <td>array</td>\n",
       "      <td>1</td>\n",
       "      <td>{'type': 'object', 'properties': {'role': {'ty...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <td>What sampling temperature to use, between 0 an...</td>\n",
       "      <td>number</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_p</th>\n",
       "      <td>An alternative to sampling with temperature, c...</td>\n",
       "      <td>number</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>How many chat completion choices to generate f...</td>\n",
       "      <td>integer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream</th>\n",
       "      <td>If set, partial message deltas will be sent, l...</td>\n",
       "      <td>boolean</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>Up to 4 sequences where the API will stop gene...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'type': 'string', 'nullable': True}, {'type'...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_tokens</th>\n",
       "      <td>The maximum number of tokens allowed for the g...</td>\n",
       "      <td>integer</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>inf</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presence_penalty</th>\n",
       "      <td>Number between -2.0 and 2.0. Positive values p...</td>\n",
       "      <td>number</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency_penalty</th>\n",
       "      <td>Number between -2.0 and 2.0. Positive values p...</td>\n",
       "      <td>number</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logit_bias</th>\n",
       "      <td>Modify the likelihood of specified tokens appe...</td>\n",
       "      <td>object</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>map</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>A unique identifier representing your end-user...</td>\n",
       "      <td>string</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>user-1234</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         description     type  \\\n",
       "model              ID of the model to use. Currently, only `gpt-3...   string   \n",
       "messages           The messages to generate chat completions for,...    array   \n",
       "temperature        What sampling temperature to use, between 0 an...   number   \n",
       "top_p              An alternative to sampling with temperature, c...   number   \n",
       "n                  How many chat completion choices to generate f...  integer   \n",
       "stream             If set, partial message deltas will be sent, l...  boolean   \n",
       "stop               Up to 4 sequences where the API will stop gene...            \n",
       "max_tokens         The maximum number of tokens allowed for the g...  integer   \n",
       "presence_penalty   Number between -2.0 and 2.0. Positive values p...   number   \n",
       "frequency_penalty  Number between -2.0 and 2.0. Positive values p...   number   \n",
       "logit_bias         Modify the likelihood of specified tokens appe...   object   \n",
       "user               A unique identifier representing your end-user...   string   \n",
       "\n",
       "                  minItems                                              items  \\\n",
       "model                                                                           \n",
       "messages                 1  {'type': 'object', 'properties': {'role': {'ty...   \n",
       "temperature                                                                     \n",
       "top_p                                                                           \n",
       "n                                                                               \n",
       "stream                                                                          \n",
       "stop                                                                            \n",
       "max_tokens                                                                      \n",
       "presence_penalty                                                                \n",
       "frequency_penalty                                                               \n",
       "logit_bias                                                                      \n",
       "user                                                                            \n",
       "\n",
       "                  minimum maximum default    example nullable  \\\n",
       "model                                                           \n",
       "messages                                                        \n",
       "temperature             0       2       1          1     True   \n",
       "top_p                   0       1       1          1     True   \n",
       "n                       1     128       1          1     True   \n",
       "stream                              False                True   \n",
       "stop                                                            \n",
       "max_tokens                            inf                       \n",
       "presence_penalty       -2       2       0                True   \n",
       "frequency_penalty      -2       2       0                True   \n",
       "logit_bias                                               True   \n",
       "user                                       user-1234            \n",
       "\n",
       "                                                               oneOf  \\\n",
       "model                                                                  \n",
       "messages                                                               \n",
       "temperature                                                            \n",
       "top_p                                                                  \n",
       "n                                                                      \n",
       "stream                                                                 \n",
       "stop               [{'type': 'string', 'nullable': True}, {'type'...   \n",
       "max_tokens                                                             \n",
       "presence_penalty                                                       \n",
       "frequency_penalty                                                      \n",
       "logit_bias                                                             \n",
       "user                                                                   \n",
       "\n",
       "                  x-oaiTypeLabel  \n",
       "model                             \n",
       "messages                          \n",
       "temperature                       \n",
       "top_p                             \n",
       "n                                 \n",
       "stream                            \n",
       "stop                              \n",
       "max_tokens                        \n",
       "presence_penalty                  \n",
       "frequency_penalty                 \n",
       "logit_bias                   map  \n",
       "user                              "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "schema = schemas['CreateChatCompletionRequest']\n",
    "chat_schema = pd.DataFrame(schema['properties']).T.fillna('')\n",
    "chat_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T13:47:08.262123Z",
     "start_time": "2023-04-22T13:47:08.236953Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model: ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported.\n",
      "\n",
      "- messages: The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction).\n",
      "\n",
      "- temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
      "\n",
      "We generally recommend altering this or `top_p` but not both.\n",
      "\n",
      "\n",
      "- top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
      "\n",
      "We generally recommend altering this or `temperature` but not both.\n",
      "\n",
      "\n",
      "- n: How many chat completion choices to generate for each input message.\n",
      "\n",
      "- stream: If set, partial message deltas will be sent, like in ChatGPT. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.\n",
      "\n",
      "\n",
      "- stop: Up to 4 sequences where the API will stop generating further tokens.\n",
      "\n",
      "\n",
      "- max_tokens: The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens).\n",
      "\n",
      "\n",
      "- presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
      "\n",
      "[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n",
      "\n",
      "\n",
      "- frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
      "\n",
      "[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n",
      "\n",
      "\n",
      "- logit_bias: Modify the likelihood of specified tokens appearing in the completion.\n",
      "\n",
      "Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n",
      "\n",
      "\n",
      "- user: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(*(f\"- {k}: {v}\" for k, v in chat_schema['description'].items()), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:07:27.996208Z",
     "start_time": "2023-04-22T15:07:27.660413Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ListEnginesResponse',\n",
       " 'ListModelsResponse',\n",
       " 'DeleteModelResponse',\n",
       " 'CreateCompletionRequest',\n",
       " 'CreateCompletionResponse',\n",
       " 'ChatCompletionRequestMessage',\n",
       " 'ChatCompletionResponseMessage',\n",
       " 'CreateChatCompletionRequest',\n",
       " 'CreateChatCompletionResponse',\n",
       " 'CreateEditRequest',\n",
       " 'CreateEditResponse',\n",
       " 'CreateImageRequest',\n",
       " 'ImagesResponse',\n",
       " 'CreateImageEditRequest',\n",
       " 'CreateImageVariationRequest',\n",
       " 'CreateModerationRequest',\n",
       " 'CreateModerationResponse',\n",
       " 'CreateSearchRequest',\n",
       " 'CreateSearchResponse',\n",
       " 'ListFilesResponse',\n",
       " 'CreateFileRequest',\n",
       " 'DeleteFileResponse',\n",
       " 'CreateAnswerRequest',\n",
       " 'CreateAnswerResponse',\n",
       " 'CreateClassificationRequest',\n",
       " 'CreateClassificationResponse',\n",
       " 'CreateFineTuneRequest',\n",
       " 'ListFineTunesResponse',\n",
       " 'ListFineTuneEventsResponse',\n",
       " 'CreateEmbeddingRequest',\n",
       " 'CreateEmbeddingResponse',\n",
       " 'CreateTranscriptionRequest',\n",
       " 'CreateTranscriptionResponse',\n",
       " 'CreateTranslationRequest',\n",
       " 'CreateTranslationResponse',\n",
       " 'Engine',\n",
       " 'Model',\n",
       " 'OpenAIFile',\n",
       " 'FineTune',\n",
       " 'FineTuneEvent']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T14:12:39.496222Z",
     "start_time": "2023-04-22T14:12:39.465176Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>type</th>\n",
       "      <th>default</th>\n",
       "      <th>nullable</th>\n",
       "      <th>oneOf</th>\n",
       "      <th>example</th>\n",
       "      <th>minimum</th>\n",
       "      <th>maximum</th>\n",
       "      <th>x-oaiTypeLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>ID of the model to use. You can use the [List ...</td>\n",
       "      <td>string</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prompt</th>\n",
       "      <td>The prompt(s) to generate completions for, enc...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;|endoftext|&gt;</td>\n",
       "      <td>True</td>\n",
       "      <td>[{'type': 'string', 'default': '', 'example': ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suffix</th>\n",
       "      <td>The suffix that comes after a completion of in...</td>\n",
       "      <td>string</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>test.</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_tokens</th>\n",
       "      <td>The maximum number of [tokens](/tokenizer) to ...</td>\n",
       "      <td>integer</td>\n",
       "      <td>16</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <td>What sampling temperature to use, between 0 an...</td>\n",
       "      <td>number</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top_p</th>\n",
       "      <td>An alternative to sampling with temperature, c...</td>\n",
       "      <td>number</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>How many completions to generate for each prom...</td>\n",
       "      <td>integer</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stream</th>\n",
       "      <td>Whether to stream back partial progress. If se...</td>\n",
       "      <td>boolean</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logprobs</th>\n",
       "      <td>Include the log probabilities on the `logprobs...</td>\n",
       "      <td>integer</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>echo</th>\n",
       "      <td>Echo back the prompt in addition to the comple...</td>\n",
       "      <td>boolean</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stop</th>\n",
       "      <td>Up to 4 sequences where the API will stop gene...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td>[{'type': 'string', 'default': '&lt;|endoftext|&gt;'...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>presence_penalty</th>\n",
       "      <td>Number between -2.0 and 2.0. Positive values p...</td>\n",
       "      <td>number</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency_penalty</th>\n",
       "      <td>Number between -2.0 and 2.0. Positive values p...</td>\n",
       "      <td>number</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>-2</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best_of</th>\n",
       "      <td>Generates `best_of` completions server-side an...</td>\n",
       "      <td>integer</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logit_bias</th>\n",
       "      <td>Modify the likelihood of specified tokens appe...</td>\n",
       "      <td>object</td>\n",
       "      <td></td>\n",
       "      <td>True</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>map</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <td>A unique identifier representing your end-user...</td>\n",
       "      <td>string</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>user-1234</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         description     type  \\\n",
       "model              ID of the model to use. You can use the [List ...   string   \n",
       "prompt             The prompt(s) to generate completions for, enc...            \n",
       "suffix             The suffix that comes after a completion of in...   string   \n",
       "max_tokens         The maximum number of [tokens](/tokenizer) to ...  integer   \n",
       "temperature        What sampling temperature to use, between 0 an...   number   \n",
       "top_p              An alternative to sampling with temperature, c...   number   \n",
       "n                  How many completions to generate for each prom...  integer   \n",
       "stream             Whether to stream back partial progress. If se...  boolean   \n",
       "logprobs           Include the log probabilities on the `logprobs...  integer   \n",
       "echo               Echo back the prompt in addition to the comple...  boolean   \n",
       "stop               Up to 4 sequences where the API will stop gene...            \n",
       "presence_penalty   Number between -2.0 and 2.0. Positive values p...   number   \n",
       "frequency_penalty  Number between -2.0 and 2.0. Positive values p...   number   \n",
       "best_of            Generates `best_of` completions server-side an...  integer   \n",
       "logit_bias         Modify the likelihood of specified tokens appe...   object   \n",
       "user               A unique identifier representing your end-user...   string   \n",
       "\n",
       "                         default nullable  \\\n",
       "model                                       \n",
       "prompt             <|endoftext|>     True   \n",
       "suffix                               True   \n",
       "max_tokens                    16     True   \n",
       "temperature                    1     True   \n",
       "top_p                          1     True   \n",
       "n                              1     True   \n",
       "stream                     False     True   \n",
       "logprobs                             True   \n",
       "echo                       False     True   \n",
       "stop                                 True   \n",
       "presence_penalty               0     True   \n",
       "frequency_penalty              0     True   \n",
       "best_of                        1     True   \n",
       "logit_bias                           True   \n",
       "user                                        \n",
       "\n",
       "                                                               oneOf  \\\n",
       "model                                                                  \n",
       "prompt             [{'type': 'string', 'default': '', 'example': ...   \n",
       "suffix                                                                 \n",
       "max_tokens                                                             \n",
       "temperature                                                            \n",
       "top_p                                                                  \n",
       "n                                                                      \n",
       "stream                                                                 \n",
       "logprobs                                                               \n",
       "echo                                                                   \n",
       "stop               [{'type': 'string', 'default': '<|endoftext|>'...   \n",
       "presence_penalty                                                       \n",
       "frequency_penalty                                                      \n",
       "best_of                                                                \n",
       "logit_bias                                                             \n",
       "user                                                                   \n",
       "\n",
       "                     example minimum maximum x-oaiTypeLabel  \n",
       "model                                                        \n",
       "prompt                                                       \n",
       "suffix                 test.                                 \n",
       "max_tokens                16       0                         \n",
       "temperature                1       0       2                 \n",
       "top_p                      1       0       1                 \n",
       "n                          1       1     128                 \n",
       "stream                                                       \n",
       "logprobs                           0       5                 \n",
       "echo                                                         \n",
       "stop                                                         \n",
       "presence_penalty                  -2       2                 \n",
       "frequency_penalty                 -2       2                 \n",
       "best_of                            0      20                 \n",
       "logit_bias                                              map  \n",
       "user               user-1234                                 "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = schemas['CreateCompletionRequest']\n",
    "completion_schema = pd.DataFrame(schema['properties']).T.fillna('')\n",
    "completion_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T14:12:49.757608Z",
     "start_time": "2023-04-22T14:12:49.729214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- model: ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.\n",
      "\n",
      "- prompt: The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\n",
      "\n",
      "Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\n",
      "\n",
      "\n",
      "- suffix: The suffix that comes after a completion of inserted text.\n",
      "\n",
      "- max_tokens: The maximum number of [tokens](/tokenizer) to generate in the completion.\n",
      "\n",
      "The token count of your prompt plus `max_tokens` cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).\n",
      "\n",
      "\n",
      "- temperature: What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
      "\n",
      "We generally recommend altering this or `top_p` but not both.\n",
      "\n",
      "\n",
      "- top_p: An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n",
      "\n",
      "We generally recommend altering this or `temperature` but not both.\n",
      "\n",
      "\n",
      "- n: How many completions to generate for each prompt.\n",
      "\n",
      "**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n",
      "\n",
      "\n",
      "- stream: Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.\n",
      "\n",
      "\n",
      "- logprobs: Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.\n",
      "\n",
      "The maximum value for `logprobs` is 5. If you need more than this, please contact us through our [Help center](https://help.openai.com) and describe your use case.\n",
      "\n",
      "\n",
      "- echo: Echo back the prompt in addition to the completion\n",
      "\n",
      "\n",
      "- stop: Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\n",
      "\n",
      "\n",
      "- presence_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n",
      "\n",
      "[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n",
      "\n",
      "\n",
      "- frequency_penalty: Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n",
      "\n",
      "[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\n",
      "\n",
      "\n",
      "- best_of: Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.\n",
      "\n",
      "When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return  `best_of` must be greater than `n`.\n",
      "\n",
      "**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n",
      "\n",
      "\n",
      "- logit_bias: Modify the likelihood of specified tokens appearing in the completion.\n",
      "\n",
      "Accepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n",
      "\n",
      "As an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated.\n",
      "\n",
      "\n",
      "- user: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(*(f\"- {k}: {v}\" for k, v in completion_schema['description'].items()), sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:00:05.825102Z",
     "start_time": "2023-04-22T15:00:05.799333Z"
    }
   },
   "outputs": [],
   "source": [
    "from oa.openai_specs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:04:02.332349Z",
     "start_time": "2023-04-22T15:04:02.309114Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(model: str, messages: List[dict], *, temperature: float = 1, top_p: float = 1, n: int = 1, stream: bool = False, stop=None, max_tokens: int = 'inf', presence_penalty: float = 0, frequency_penalty: float = 0, logit_bias: dict = None, user: str)\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(schema_to_signature(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-22T15:00:55.645217Z",
     "start_time": "2023-04-22T15:00:55.619004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'description': 'ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models/overview) for descriptions of them.',\n",
       "  'type': 'string'},\n",
       " 'prompt': {'description': 'The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\\n\\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\\n',\n",
       "  'default': '<|endoftext|>',\n",
       "  'nullable': True,\n",
       "  'oneOf': [{'type': 'string', 'default': '', 'example': 'This is a test.'},\n",
       "   {'type': 'array',\n",
       "    'items': {'type': 'string', 'default': '', 'example': 'This is a test.'}},\n",
       "   {'type': 'array',\n",
       "    'minItems': 1,\n",
       "    'items': {'type': 'integer'},\n",
       "    'example': '[1212, 318, 257, 1332, 13]'},\n",
       "   {'type': 'array',\n",
       "    'minItems': 1,\n",
       "    'items': {'type': 'array', 'minItems': 1, 'items': {'type': 'integer'}},\n",
       "    'example': '[[1212, 318, 257, 1332, 13]]'}]},\n",
       " 'suffix': {'description': 'The suffix that comes after a completion of inserted text.',\n",
       "  'default': None,\n",
       "  'nullable': True,\n",
       "  'type': 'string',\n",
       "  'example': 'test.'},\n",
       " 'max_tokens': {'type': 'integer',\n",
       "  'minimum': 0,\n",
       "  'default': 16,\n",
       "  'example': 16,\n",
       "  'nullable': True,\n",
       "  'description': \"The maximum number of [tokens](/tokenizer) to generate in the completion.\\n\\nThe token count of your prompt plus `max_tokens` cannot exceed the model's context length. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).\\n\"},\n",
       " 'temperature': {'type': 'number',\n",
       "  'minimum': 0,\n",
       "  'maximum': 2,\n",
       "  'default': 1,\n",
       "  'example': 1,\n",
       "  'nullable': True,\n",
       "  'description': 'What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\\n\\nWe generally recommend altering this or `top_p` but not both.\\n'},\n",
       " 'top_p': {'type': 'number',\n",
       "  'minimum': 0,\n",
       "  'maximum': 1,\n",
       "  'default': 1,\n",
       "  'example': 1,\n",
       "  'nullable': True,\n",
       "  'description': 'An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\\n\\nWe generally recommend altering this or `temperature` but not both.\\n'},\n",
       " 'n': {'type': 'integer',\n",
       "  'minimum': 1,\n",
       "  'maximum': 128,\n",
       "  'default': 1,\n",
       "  'example': 1,\n",
       "  'nullable': True,\n",
       "  'description': 'How many completions to generate for each prompt.\\n\\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\\n'},\n",
       " 'stream': {'description': 'Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message.\\n',\n",
       "  'type': 'boolean',\n",
       "  'nullable': True,\n",
       "  'default': False},\n",
       " 'logprobs': {'type': 'integer',\n",
       "  'minimum': 0,\n",
       "  'maximum': 5,\n",
       "  'default': None,\n",
       "  'nullable': True,\n",
       "  'description': 'Include the log probabilities on the `logprobs` most likely tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.\\n\\nThe maximum value for `logprobs` is 5. If you need more than this, please contact us through our [Help center](https://help.openai.com) and describe your use case.\\n'},\n",
       " 'echo': {'type': 'boolean',\n",
       "  'default': False,\n",
       "  'nullable': True,\n",
       "  'description': 'Echo back the prompt in addition to the completion\\n'},\n",
       " 'stop': {'description': 'Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.\\n',\n",
       "  'default': None,\n",
       "  'nullable': True,\n",
       "  'oneOf': [{'type': 'string',\n",
       "    'default': '<|endoftext|>',\n",
       "    'example': '\\n',\n",
       "    'nullable': True},\n",
       "   {'type': 'array',\n",
       "    'minItems': 1,\n",
       "    'maxItems': 4,\n",
       "    'items': {'type': 'string', 'example': '[\"\\\\n\"]'}}]},\n",
       " 'presence_penalty': {'type': 'number',\n",
       "  'default': 0,\n",
       "  'minimum': -2,\n",
       "  'maximum': 2,\n",
       "  'nullable': True,\n",
       "  'description': \"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\\n\\n[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\\n\"},\n",
       " 'frequency_penalty': {'type': 'number',\n",
       "  'default': 0,\n",
       "  'minimum': -2,\n",
       "  'maximum': 2,\n",
       "  'nullable': True,\n",
       "  'description': \"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\\n\\n[See more information about frequency and presence penalties.](/docs/api-reference/parameter-details)\\n\"},\n",
       " 'best_of': {'type': 'integer',\n",
       "  'default': 1,\n",
       "  'minimum': 0,\n",
       "  'maximum': 20,\n",
       "  'nullable': True,\n",
       "  'description': 'Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.\\n\\nWhen used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return  `best_of` must be greater than `n`.\\n\\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\\n'},\n",
       " 'logit_bias': {'type': 'object',\n",
       "  'x-oaiTypeLabel': 'map',\n",
       "  'default': None,\n",
       "  'nullable': True,\n",
       "  'description': 'Modify the likelihood of specified tokens appearing in the completion.\\n\\nAccepts a json object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) (which works for both GPT-2 and GPT-3) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\\n\\nAs an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated.\\n'},\n",
       " 'user': {'type': 'string',\n",
       "  'example': 'user-1234',\n",
       "  'description': 'A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids).\\n'}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T08:05:49.352582Z",
     "start_time": "2023-03-27T08:05:49.328212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/engines',\n",
       " '/engines/{engine_id}',\n",
       " '/completions',\n",
       " '/chat/completions',\n",
       " '/edits',\n",
       " '/images/generations',\n",
       " '/images/edits',\n",
       " '/images/variations',\n",
       " '/embeddings',\n",
       " '/audio/transcriptions',\n",
       " '/audio/translations',\n",
       " '/engines/{engine_id}/search',\n",
       " '/files',\n",
       " '/files/{file_id}',\n",
       " '/files/{file_id}/content',\n",
       " '/answers',\n",
       " '/classifications',\n",
       " '/fine-tunes',\n",
       " '/fine-tunes/{fine_tune_id}',\n",
       " '/fine-tunes/{fine_tune_id}/cancel',\n",
       " '/fine-tunes/{fine_tune_id}/events',\n",
       " '/models',\n",
       " '/models/{model}',\n",
       " '/moderations']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(specs['paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T07:57:05.400226Z",
     "start_time": "2023-03-27T07:57:05.379075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model                {'description': 'ID of the model to use. Curre...\n",
      "messages             {'description': 'The messages to generate chat...\n",
      "temperature          {'type': 'number', 'minimum': 0, 'maximum': 2,...\n",
      "top_p                {'type': 'number', 'minimum': 0, 'maximum': 1,...\n",
      "n                    {'type': 'integer', 'minimum': 1, 'maximum': 1...\n",
      "stream               {'description': 'If set, partial message delta...\n",
      "stop                 {'description': 'Up to 4 sequences where the A...\n",
      "max_tokens           {'description': 'The maximum number of tokens ...\n",
      "presence_penalty     {'type': 'number', 'default': 0, 'minimum': -2...\n",
      "frequency_penalty    {'type': 'number', 'default': 0, 'minimum': -2...\n",
      "logit_bias           {'type': 'object', 'x-oaiTypeLabel': 'map', 'd...\n",
      "user                 {'type': 'string', 'example': 'user-1234', 'de...\n",
      "dtype: object\n",
      "['model', 'messages']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T07:53:28.271497Z",
     "start_time": "2023-03-27T07:53:28.252176Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T07:44:17.516750Z",
     "start_time": "2023-03-27T07:44:17.503486Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections.abc import MutableMapping\n",
    "\n",
    "def flatten_dict(d: MutableMapping, parent_key: str = '', sep: str ='.') -> MutableMapping:\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = parent_key + sep + k if parent_key else k\n",
    "        if isinstance(v, MutableMapping):\n",
    "            items.extend(flatten_dict(v, new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T07:44:20.074690Z",
     "start_time": "2023-03-27T07:44:20.050856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1215"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dol import flatten\n",
    "\n",
    "tt = flatten_dict(t)\n",
    "len(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T14:01:08.480927Z",
     "start_time": "2023-03-31T14:01:08.466494Z"
    }
   },
   "outputs": [],
   "source": [
    "# list(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T11:46:52.221788Z",
     "start_time": "2023-03-07T11:46:52.201615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get': {'operationId': 'listEngines',\n",
       "  'deprecated': True,\n",
       "  'tags': ['OpenAI'],\n",
       "  'summary': 'Lists the currently available (non-finetuned) models, and provides basic information about each one such as the owner and availability.',\n",
       "  'responses': {'200': {'description': 'OK',\n",
       "    'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListEnginesResponse'}}}}},\n",
       "  'x-oaiMeta': {'name': 'List engines',\n",
       "   'group': 'engines',\n",
       "   'path': 'list',\n",
       "   'examples': {'curl': \"curl https://api.openai.com/v1/engines \\\\\\n  -H 'Authorization: Bearer YOUR_API_KEY'\\n\",\n",
       "    'python': 'import os\\nimport openai\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\nopenai.Engine.list()\\n',\n",
       "    'node.js': 'const { Configuration, OpenAIApi } = require(\"openai\");\\nconst configuration = new Configuration({\\n  apiKey: process.env.OPENAI_API_KEY,\\n});\\nconst openai = new OpenAIApi(configuration);\\nconst response = await openai.listEngines();\\n'},\n",
       "   'response': '{\\n  \"data\": [\\n    {\\n      \"id\": \"engine-id-0\",\\n      \"object\": \"engine\",\\n      \"owner\": \"organization-owner\",\\n      \"ready\": true\\n    },\\n    {\\n      \"id\": \"engine-id-2\",\\n      \"object\": \"engine\",\\n      \"owner\": \"organization-owner\",\\n      \"ready\": true\\n    },\\n    {\\n      \"id\": \"engine-id-3\",\\n      \"object\": \"engine\",\\n      \"owner\": \"openai\",\\n      \"ready\": false\\n    },\\n  ],\\n  \"object\": \"list\"\\n}\\n'}}}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t['paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T12:01:24.823457Z",
     "start_time": "2023-03-07T12:01:24.807777Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get': {'operationId': 'listEngines',\n",
       "  'deprecated': True,\n",
       "  'tags': ['OpenAI'],\n",
       "  'summary': 'Lists the currently available (non-finetuned) models, and provides basic information about each one such as the owner and availability.',\n",
       "  'responses': {'200': {'description': 'OK',\n",
       "    'content': {'application/json': {'schema': {'$ref': '#/components/schemas/ListEnginesResponse'}}}}},\n",
       "  'x-oaiMeta': {'name': 'List engines',\n",
       "   'group': 'engines',\n",
       "   'path': 'list',\n",
       "   'examples': {'curl': \"curl https://api.openai.com/v1/engines \\\\\\n  -H 'Authorization: Bearer YOUR_API_KEY'\\n\",\n",
       "    'python': 'import os\\nimport openai\\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\\nopenai.Engine.list()\\n',\n",
       "    'node.js': 'const { Configuration, OpenAIApi } = require(\"openai\");\\nconst configuration = new Configuration({\\n  apiKey: process.env.OPENAI_API_KEY,\\n});\\nconst openai = new OpenAIApi(configuration);\\nconst response = await openai.listEngines();\\n'},\n",
       "   'response': '{\\n  \"data\": [\\n    {\\n      \"id\": \"engine-id-0\",\\n      \"object\": \"engine\",\\n      \"owner\": \"organization-owner\",\\n      \"ready\": true\\n    },\\n    {\\n      \"id\": \"engine-id-2\",\\n      \"object\": \"engine\",\\n      \"owner\": \"organization-owner\",\\n      \"ready\": true\\n    },\\n    {\\n      \"id\": \"engine-id-3\",\\n      \"object\": \"engine\",\\n      \"owner\": \"openai\",\\n      \"ready\": false\\n    },\\n  ],\\n  \"object\": \"list\"\\n}\\n'}}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dol import path_get\n",
    "\n",
    "path_get(t, 'paths./engines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine/Model list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T11:22:10.939187Z",
     "start_time": "2023-03-08T11:22:10.534374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APIError Audio ChatCompletion Completion ContextVar Customer Deployment Edit Embedding Engine ErrorObject File FineTune Image InvalidRequestError Model Moderation OpenAIError Optional TYPE_CHECKING __all__ __annotations__ __builtins__ __cached__ __doc__ __file__ __loader__ __name__ __package__ __path__ __spec__ aiosession api_base api_key api_key_path api_requestor api_resources api_type api_version app_info ca_bundle_path datalib debug enable_telemetry error log object_classes openai_object openai_response organization os proxy util verify_ssl_certs version\n"
     ]
    }
   ],
   "source": [
    "from oa import openai\n",
    "from glom import glom\n",
    "print(*dir(openai))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T11:21:28.980647Z",
     "start_time": "2023-03-08T11:21:28.414367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__class__ __class_getitem__ __contains__ __copy__ __deepcopy__ __delattr__ __delitem__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattr__ __getattribute__ __getitem__ __gt__ __hash__ __init__ __init_subclass__ __ior__ __iter__ __le__ __len__ __lt__ __module__ __ne__ __new__ __or__ __reduce__ __reduce_ex__ __repr__ __reversed__ __ror__ __setattr__ __setitem__ __setstate__ __sizeof__ __str__ __subclasshook__ __weakref__ _previous _response_ms _retrieve_params api_base api_base_override api_key api_type api_version arequest clear construct_from copy engine fromkeys get items keys openai_id organization pop popitem refresh_from request response_ms setdefault to_dict to_dict_recursive typed_api_type update values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['object', 'data']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = openai.Model.list()\n",
    "print(*dir(t))\n",
    "list(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T11:28:27.665719Z",
     "start_time": "2023-03-08T11:28:27.647262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['babbage',\n",
       " 'davinci',\n",
       " 'gpt-3.5-turbo-0301',\n",
       " 'text-davinci-003',\n",
       " 'babbage-code-search-code',\n",
       " 'text-similarity-babbage-001',\n",
       " 'text-davinci-001',\n",
       " 'ada',\n",
       " 'curie-instruct-beta',\n",
       " 'babbage-code-search-text',\n",
       " 'babbage-similarity',\n",
       " 'code-search-babbage-text-001',\n",
       " 'text-embedding-ada-002',\n",
       " 'code-cushman-001',\n",
       " 'whisper-1',\n",
       " 'gpt-3.5-turbo',\n",
       " 'code-search-babbage-code-001',\n",
       " 'audio-transcribe-deprecated',\n",
       " 'text-ada-001',\n",
       " 'text-similarity-ada-001',\n",
       " 'text-davinci-insert-002',\n",
       " 'ada-code-search-code',\n",
       " 'ada-similarity',\n",
       " 'code-search-ada-text-001',\n",
       " 'text-search-ada-query-001',\n",
       " 'text-curie-001',\n",
       " 'text-davinci-edit-001',\n",
       " 'davinci-search-document',\n",
       " 'ada-code-search-text',\n",
       " 'text-search-ada-doc-001',\n",
       " 'code-davinci-edit-001',\n",
       " 'davinci-instruct-beta',\n",
       " 'text-similarity-curie-001',\n",
       " 'code-search-ada-code-001',\n",
       " 'ada-search-query',\n",
       " 'text-search-davinci-query-001',\n",
       " 'curie-search-query',\n",
       " 'davinci-search-query',\n",
       " 'text-davinci-insert-001',\n",
       " 'babbage-search-document',\n",
       " 'ada-search-document',\n",
       " 'text-search-curie-query-001',\n",
       " 'text-search-babbage-doc-001',\n",
       " 'text-davinci-002',\n",
       " 'curie-search-document',\n",
       " 'text-search-curie-doc-001',\n",
       " 'babbage-search-query',\n",
       " 'text-babbage-001',\n",
       " 'text-search-davinci-doc-001',\n",
       " 'code-davinci-002',\n",
       " 'text-search-babbage-query-001',\n",
       " 'curie-similarity',\n",
       " 'curie',\n",
       " 'text-similarity-davinci-001',\n",
       " 'davinci-similarity',\n",
       " 'cushman:2020-05-03',\n",
       " 'ada:2020-05-03',\n",
       " 'babbage:2020-05-03',\n",
       " 'curie:2020-05-03',\n",
       " 'davinci:2020-05-03',\n",
       " 'if-davinci-v2',\n",
       " 'if-curie-v2',\n",
       " 'if-davinci:3.0.0',\n",
       " 'davinci-if:3.0.0',\n",
       " 'davinci-instruct-beta:2.0.0',\n",
       " 'text-ada:001',\n",
       " 'text-davinci:001',\n",
       " 'text-curie:001',\n",
       " 'text-babbage:001']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# glom(t['data'], '*.root')\n",
    "import glom\n",
    "field = 'root'\n",
    "tt = t['data']\n",
    "# glom.glom(tt, glom.Spec(field))\n",
    "from functools import partial\n",
    "field_extractor = lambda field: partial(\n",
    "    map, \n",
    "    partial(glom.glom, spec=glom.Spec(field))\n",
    ")\n",
    "f = field_extractor('root')\n",
    "list(f(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-08T11:21:24.219065Z",
     "start_time": "2023-03-08T11:21:23.824218Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__class__ __class_getitem__ __contains__ __copy__ __deepcopy__ __delattr__ __delitem__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattr__ __getattribute__ __getitem__ __gt__ __hash__ __init__ __init_subclass__ __ior__ __iter__ __le__ __len__ __lt__ __module__ __ne__ __new__ __or__ __reduce__ __reduce_ex__ __repr__ __reversed__ __ror__ __setattr__ __setitem__ __setstate__ __sizeof__ __str__ __subclasshook__ __weakref__ _previous _response_ms _retrieve_params api_base api_base_override api_key api_type api_version arequest clear construct_from copy engine fromkeys get items keys openai_id organization pop popitem refresh_from request response_ms setdefault to_dict to_dict_recursive typed_api_type update values\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['object', 'data']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = openai.Engine.list()\n",
    "print(*dir(t))\n",
    "list(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T10:34:02.052131Z",
     "start_time": "2023-03-07T10:34:01.984217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object</th>\n",
       "      <th>id</th>\n",
       "      <th>ready</th>\n",
       "      <th>owner</th>\n",
       "      <th>permissions</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>engine</td>\n",
       "      <td>babbage</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>engine</td>\n",
       "      <td>davinci</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>engine</td>\n",
       "      <td>gpt-3.5-turbo-0301</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-davinci-003</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-internal</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>engine</td>\n",
       "      <td>babbage-code-search-code</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-similarity-babbage-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-davinci-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>engine</td>\n",
       "      <td>ada</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>engine</td>\n",
       "      <td>curie-instruct-beta</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>engine</td>\n",
       "      <td>babbage-code-search-text</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>engine</td>\n",
       "      <td>babbage-similarity</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>engine</td>\n",
       "      <td>gpt-3.5-turbo</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>engine</td>\n",
       "      <td>code-davinci-002</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>engine</td>\n",
       "      <td>code-search-babbage-text-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-embedding-ada-002</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-internal</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>engine</td>\n",
       "      <td>code-cushman-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>engine</td>\n",
       "      <td>whisper-1</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-internal</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>engine</td>\n",
       "      <td>code-search-babbage-code-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>engine</td>\n",
       "      <td>audio-transcribe-deprecated</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-internal</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-ada-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-similarity-ada-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-davinci-insert-002</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>engine</td>\n",
       "      <td>ada-code-search-code</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>engine</td>\n",
       "      <td>ada-similarity</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>engine</td>\n",
       "      <td>code-search-ada-text-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-search-ada-query-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-curie-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-davinci-edit-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>engine</td>\n",
       "      <td>davinci-search-document</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>engine</td>\n",
       "      <td>ada-code-search-text</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-search-ada-doc-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>engine</td>\n",
       "      <td>code-davinci-edit-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>engine</td>\n",
       "      <td>davinci-instruct-beta</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-similarity-curie-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>engine</td>\n",
       "      <td>code-search-ada-code-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>engine</td>\n",
       "      <td>ada-search-query</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-search-davinci-query-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>engine</td>\n",
       "      <td>curie-search-query</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>engine</td>\n",
       "      <td>davinci-search-query</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-davinci-insert-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>engine</td>\n",
       "      <td>babbage-search-document</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>engine</td>\n",
       "      <td>ada-search-document</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-search-curie-query-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-search-babbage-doc-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-davinci-002</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>engine</td>\n",
       "      <td>curie-search-document</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-search-curie-doc-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>engine</td>\n",
       "      <td>babbage-search-query</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-babbage-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-search-davinci-doc-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-search-babbage-query-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>engine</td>\n",
       "      <td>curie-similarity</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>engine</td>\n",
       "      <td>curie</td>\n",
       "      <td>True</td>\n",
       "      <td>openai</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>engine</td>\n",
       "      <td>text-similarity-davinci-001</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>engine</td>\n",
       "      <td>davinci-similarity</td>\n",
       "      <td>True</td>\n",
       "      <td>openai-dev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    object                             id  ...  permissions created\n",
       "0   engine                        babbage  ...         None    None\n",
       "1   engine                        davinci  ...         None    None\n",
       "2   engine             gpt-3.5-turbo-0301  ...         None    None\n",
       "3   engine               text-davinci-003  ...         None    None\n",
       "4   engine       babbage-code-search-code  ...         None    None\n",
       "5   engine    text-similarity-babbage-001  ...         None    None\n",
       "6   engine               text-davinci-001  ...         None    None\n",
       "7   engine                            ada  ...         None    None\n",
       "8   engine            curie-instruct-beta  ...         None    None\n",
       "9   engine       babbage-code-search-text  ...         None    None\n",
       "10  engine             babbage-similarity  ...         None    None\n",
       "11  engine                  gpt-3.5-turbo  ...         None    None\n",
       "12  engine               code-davinci-002  ...         None    None\n",
       "13  engine   code-search-babbage-text-001  ...         None    None\n",
       "14  engine         text-embedding-ada-002  ...         None    None\n",
       "15  engine               code-cushman-001  ...         None    None\n",
       "16  engine                      whisper-1  ...         None    None\n",
       "17  engine   code-search-babbage-code-001  ...         None    None\n",
       "18  engine    audio-transcribe-deprecated  ...         None    None\n",
       "19  engine                   text-ada-001  ...         None    None\n",
       "20  engine        text-similarity-ada-001  ...         None    None\n",
       "21  engine        text-davinci-insert-002  ...         None    None\n",
       "22  engine           ada-code-search-code  ...         None    None\n",
       "23  engine                 ada-similarity  ...         None    None\n",
       "24  engine       code-search-ada-text-001  ...         None    None\n",
       "25  engine      text-search-ada-query-001  ...         None    None\n",
       "26  engine                 text-curie-001  ...         None    None\n",
       "27  engine          text-davinci-edit-001  ...         None    None\n",
       "28  engine        davinci-search-document  ...         None    None\n",
       "29  engine           ada-code-search-text  ...         None    None\n",
       "30  engine        text-search-ada-doc-001  ...         None    None\n",
       "31  engine          code-davinci-edit-001  ...         None    None\n",
       "32  engine          davinci-instruct-beta  ...         None    None\n",
       "33  engine      text-similarity-curie-001  ...         None    None\n",
       "34  engine       code-search-ada-code-001  ...         None    None\n",
       "35  engine               ada-search-query  ...         None    None\n",
       "36  engine  text-search-davinci-query-001  ...         None    None\n",
       "37  engine             curie-search-query  ...         None    None\n",
       "38  engine           davinci-search-query  ...         None    None\n",
       "39  engine        text-davinci-insert-001  ...         None    None\n",
       "40  engine        babbage-search-document  ...         None    None\n",
       "41  engine            ada-search-document  ...         None    None\n",
       "42  engine    text-search-curie-query-001  ...         None    None\n",
       "43  engine    text-search-babbage-doc-001  ...         None    None\n",
       "44  engine               text-davinci-002  ...         None    None\n",
       "45  engine          curie-search-document  ...         None    None\n",
       "46  engine      text-search-curie-doc-001  ...         None    None\n",
       "47  engine           babbage-search-query  ...         None    None\n",
       "48  engine               text-babbage-001  ...         None    None\n",
       "49  engine    text-search-davinci-doc-001  ...         None    None\n",
       "50  engine  text-search-babbage-query-001  ...         None    None\n",
       "51  engine               curie-similarity  ...         None    None\n",
       "52  engine                          curie  ...         None    None\n",
       "53  engine    text-similarity-davinci-001  ...         None    None\n",
       "54  engine             davinci-similarity  ...         None    None\n",
       "\n",
       "[55 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tt = t.to_dict_recursive()\n",
    "pd.DataFrame(tt['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## illustrating a story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T09:21:13.978013Z",
     "start_time": "2023-05-07T09:21:13.969030Z"
    }
   },
   "outputs": [],
   "source": [
    "import oa.examples.illustrate_stories as ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-07T09:21:37.960058Z",
     "start_time": "2023-05-07T09:21:20.381344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Once upon a time, there lived a girl \n",
      "Red Riding Hood and her hood so curled \n",
      "It twirled and twirled with each twirl\n",
      "As she travelled through the forest so green, she smiled \n",
      "\n",
      "The sun shone brightly, atop the trees in town \n",
      "But a wolf was waiting and Red never saw his frown \n",
      "He followed her closely, unseen behind the trees \n",
      "As Red rode on, she failed to notice\n",
      "\n",
      "The big bad wolf he thought with glee \n",
      "Surely I will have that Red Riding Hood for dinner, not me! \n",
      "He thought this plan would be so great\n",
      "So he followed Red Riding Hood and his belly opened to bait \n",
      "\n",
      "But Red heard a howl and quickened her pace \n",
      "She peered behind and saw the wolf's face \n",
      "She ran as fast as she could throughout the day\n",
      "Til finally, Red's granny spotter her, and she was saved \n",
      "\n",
      "The wolf's plan was foiled, for he had not a chance \n",
      "Red wise was entertained and got to keep her lunch \n",
      "From then on, Red stayed away from the forest so green \n",
      "But she still kept her hood so curled, with joy so serene.\n"
     ]
    }
   ],
   "source": [
    "story_text = ii.make_it_rhyming('Red riding hood')\n",
    "print(story_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T14:54:13.637743Z",
     "start_time": "2023-03-31T14:54:12.044655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red riding hood is standing outside her grandma's house, wearing a red cape and hood. She knocks on the door, and her grandma says \"come in\". Red riding hood takes off her cape and hood, and her grandma gives her a big hug.\n"
     ]
    }
   ],
   "source": [
    "image_description = ii.get_image_description(story_text)\n",
    "print(image_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T14:54:18.633873Z",
     "start_time": "2023-03-31T14:54:13.646766Z"
    }
   },
   "outputs": [],
   "source": [
    "image_url = ii.get_image_url(image_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T14:54:18.662856Z",
     "start_time": "2023-03-31T14:54:18.638925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "    <body>\n",
       "    <img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-JfBXG6inGZl71VTgxh8IJPZ0.png?st=2023-03-31T13%3A54%3A18Z&se=2023-03-31T15%3A54%3A18Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-03-30T15%3A54%3A53Z&ske=2023-03-31T15%3A54%3A53Z&sks=b&skv=2021-08-06&sig=D6JEMJ5n1ySJ7pewC30wdq%2BIJYou3CvDWDTVDzLsyug%3D\" />\n",
       "    <p><p>Red riding hood went to visit her grandma<br>She was wearing a red cape and a red hood<br>She knocked on the door and her grandma said &quot;come in&quot;<br>Red riding hood took off her red cape and her red hood<br>And her grandma gave her a big hug</p></p>\n",
       "    </body>\n",
       "    </html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(ii.aggregate_story_and_image(image_url, story_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T14:57:24.804862Z",
     "start_time": "2023-03-31T14:57:24.691613Z"
    }
   },
   "outputs": [],
   "source": [
    "from plunk.tw.i2i_demos.play.resources import make_children_story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-31T14:57:31.842952Z",
     "start_time": "2023-03-31T14:57:29.720473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 7.1.0 (20230121.1956)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"223pt\" height=\"476pt\"\n",
       " viewBox=\"0.00 0.00 222.50 476.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 472)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-472 218.5,-472 218.5,4 -4,4\"/>\n",
       "<!-- rhyming_story -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>rhyming_story</title>\n",
       "<text text-anchor=\"middle\" x=\"157.5\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">rhyming_story</text>\n",
       "</g>\n",
       "<!-- get_illustration -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>get_illustration</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"129.5,-252 31.5,-252 31.5,-216 129.5,-216 129.5,-252\"/>\n",
       "<text text-anchor=\"middle\" x=\"80.5\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">get_illustration</text>\n",
       "</g>\n",
       "<!-- rhyming_story&#45;&gt;get_illustration -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>rhyming_story&#45;&gt;get_illustration</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M138.86,-288.05C129.54,-279.58 118.08,-269.17 107.8,-259.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110.38,-257.43 100.62,-253.29 105.67,-262.61 110.38,-257.43\"/>\n",
       "</g>\n",
       "<!-- aggregate_story_and_image -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>aggregate_story_and_image</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"203.5,-108 33.5,-108 33.5,-72 203.5,-72 203.5,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">aggregate_story_and_image</text>\n",
       "</g>\n",
       "<!-- rhyming_story&#45;&gt;aggregate_story_and_image -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>rhyming_story&#45;&gt;aggregate_story_and_image</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.44,-288.21C147.78,-251.67 131.91,-164.59 123.67,-119.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"127.16,-119.01 121.93,-109.8 120.28,-120.26 127.16,-119.01\"/>\n",
       "</g>\n",
       "<!-- story -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>story</title>\n",
       "<text text-anchor=\"middle\" x=\"157.5\" y=\"-446.3\" font-family=\"Times,serif\" font-size=\"14.00\">story</text>\n",
       "</g>\n",
       "<!-- make_it_rhyming -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>make_it_rhyming</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"214.5,-396 100.5,-396 100.5,-360 214.5,-360 214.5,-396\"/>\n",
       "<text text-anchor=\"middle\" x=\"157.5\" y=\"-374.3\" font-family=\"Times,serif\" font-size=\"14.00\">make_it_rhyming</text>\n",
       "</g>\n",
       "<!-- story&#45;&gt;make_it_rhyming -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>story&#45;&gt;make_it_rhyming</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M157.5,-432.05C157.5,-424.68 157.5,-415.84 157.5,-407.51\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"161,-407.79 157.5,-397.79 154,-407.79 161,-407.79\"/>\n",
       "</g>\n",
       "<!-- make_it_rhyming&#45;&gt;rhyming_story -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>make_it_rhyming&#45;&gt;rhyming_story</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M157.5,-359.7C157.5,-352.21 157.5,-343.26 157.5,-334.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"161,-335.14 157.5,-325.14 154,-335.14 161,-335.14\"/>\n",
       "</g>\n",
       "<!-- image -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>image</title>\n",
       "<text text-anchor=\"middle\" x=\"91.5\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">image</text>\n",
       "</g>\n",
       "<!-- image&#45;&gt;aggregate_story_and_image -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>image&#45;&gt;aggregate_story_and_image</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M98.04,-144.05C100.94,-136.52 104.45,-127.44 107.72,-118.94\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"110.96,-120.28 111.29,-109.69 104.43,-117.76 110.96,-120.28\"/>\n",
       "</g>\n",
       "<!-- image_style -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>image_style</title>\n",
       "<text text-anchor=\"middle\" x=\"45.5\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">image_style=</text>\n",
       "</g>\n",
       "<!-- image_style&#45;&gt;get_illustration -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>image_style&#45;&gt;get_illustration</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.97,-288.05C57.83,-280.35 62.49,-271.03 66.82,-262.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69.84,-264.14 71.19,-253.63 63.58,-261.01 69.84,-264.14\"/>\n",
       "</g>\n",
       "<!-- get_illustration&#45;&gt;image -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>get_illustration&#45;&gt;image</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M83.22,-215.7C84.4,-208.21 85.8,-199.26 87.12,-190.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.56,-191.54 88.65,-181.12 83.64,-190.45 90.56,-191.54\"/>\n",
       "</g>\n",
       "<!-- page -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>page</title>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">page</text>\n",
       "</g>\n",
       "<!-- aggregate_story_and_image&#45;&gt;page -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>aggregate_story_and_image&#45;&gt;page</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118.5,-71.7C118.5,-64.21 118.5,-55.26 118.5,-46.88\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122,-47.14 118.5,-37.14 115,-47.14 122,-47.14\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x118f53a00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_children_story.dot_digraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-03T14:49:08.198762Z",
     "start_time": "2023-04-03T14:49:08.191118Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# HTML(make_children_story('hansel and gretel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making an Aesop's Fables book"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: https://aesopfables.com/aesopsel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "from tabled import get_tables_from_url\n",
    "from dol import Files, wrap_kvs, mk_dirs_if_missing\n",
    "\n",
    "\n",
    "root_url = 'https://aesopfables.com/'\n",
    "url = root_url + 'aesopsel.html'\n",
    "\n",
    "def _clean_up_fable_table(t):\n",
    "    t.columns = ['fable', 'moral']\n",
    "    t['moral'] = t['moral'].map(lambda x: x[0].strip())\n",
    "    t['moral'] = t['moral'].map(lambda x: x[1:] if x.startswith('.') else x)\n",
    "    t['title'], t['rel_url'] = zip(*t['fable'])\n",
    "    t['url'] = t['rel_url'].map(lambda x: root_url + x)\n",
    "    del t['fable']\n",
    "    return t\n",
    "\n",
    "def get_fable_table(files):\n",
    "    if 'fables.csv' not in files:\n",
    "        df = get_tables_from_url(url, extract_links='all')[0]\n",
    "        df = _clean_up_fable_table(df)\n",
    "        files['fables.csv'] = df.to_csv(index=False).encode()\n",
    "    return pd.read_csv(io.BytesIO(files['fables.csv']))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moral</th>\n",
       "      <th>title</th>\n",
       "      <th>rel_url</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Appearances are deceptive</td>\n",
       "      <td>The Ant and the Chrysalis</td>\n",
       "      <td>/cgi/aesop1.cgi?sel&amp;TheAntandtheChrysalis</td>\n",
       "      <td>https://aesopfables.com//cgi/aesop1.cgi?sel&amp;Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One good turn deserves another</td>\n",
       "      <td>The Ant and the Dove</td>\n",
       "      <td>/cgi/aesop1.cgi?sel&amp;TheAntandtheDove&amp;&amp;antdove.ram</td>\n",
       "      <td>https://aesopfables.com//cgi/aesop1.cgi?sel&amp;Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is best to prepare for the days of necessity</td>\n",
       "      <td>The Ant and the Grasshopper</td>\n",
       "      <td>/cgi/aesop1.cgi?sel&amp;TheAntandtheGrasshopper&amp;&amp;a...</td>\n",
       "      <td>https://aesopfables.com//cgi/aesop1.cgi?sel&amp;Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He that finds discontentment in one place is n...</td>\n",
       "      <td>The Ass and His Masters</td>\n",
       "      <td>/cgi/aesop1.cgi?sel&amp;TheAssandHisMasters</td>\n",
       "      <td>https://aesopfables.com//cgi/aesop1.cgi?sel&amp;Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A man is known by the company he keeps</td>\n",
       "      <td>The Ass and his Purchaser</td>\n",
       "      <td>/cgi/aesop1.cgi?sel&amp;TheAssandhisPurchaser2</td>\n",
       "      <td>https://aesopfables.com//cgi/aesop1.cgi?sel&amp;Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Every man for himself</td>\n",
       "      <td>The Three Tradesmen</td>\n",
       "      <td>/cgi/aesop1.cgi?sel&amp;TheThreeTradesmen</td>\n",
       "      <td>https://aesopfables.com//cgi/aesop1.cgi?sel&amp;Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Quality is better than quantity</td>\n",
       "      <td>The Vixen and the Lioness</td>\n",
       "      <td>/cgi/aesop1.cgi?sel&amp;TheVixenandtheLioness</td>\n",
       "      <td>https://aesopfables.com//cgi/aesop1.cgi?sel&amp;Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>It is easy to be brave from a safe distance</td>\n",
       "      <td>The Wolf and the Kid</td>\n",
       "      <td>/cgi/aesop1.cgi?sel&amp;TheWolfandtheKid</td>\n",
       "      <td>https://aesopfables.com//cgi/aesop1.cgi?sel&amp;Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Appearances are deceptive</td>\n",
       "      <td>The Wolf in Sheep's Clothing</td>\n",
       "      <td>/cgi/aesop1.cgi?sel&amp;TheWolfinSheepsClothing2</td>\n",
       "      <td>https://aesopfables.com//cgi/aesop1.cgi?sel&amp;Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>No gratitude from the wicked</td>\n",
       "      <td>The Woodman and the Serpent</td>\n",
       "      <td>/cgi/aesop1.cgi?sel&amp;TheWoodmanandtheSerpent</td>\n",
       "      <td>https://aesopfables.com//cgi/aesop1.cgi?sel&amp;Th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>86 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                moral  \\\n",
       "0                           Appearances are deceptive   \n",
       "1                      One good turn deserves another   \n",
       "2     It is best to prepare for the days of necessity   \n",
       "3   He that finds discontentment in one place is n...   \n",
       "4              A man is known by the company he keeps   \n",
       "..                                                ...   \n",
       "81                              Every man for himself   \n",
       "82                    Quality is better than quantity   \n",
       "83        It is easy to be brave from a safe distance   \n",
       "84                          Appearances are deceptive   \n",
       "85                       No gratitude from the wicked   \n",
       "\n",
       "                           title  \\\n",
       "0      The Ant and the Chrysalis   \n",
       "1           The Ant and the Dove   \n",
       "2    The Ant and the Grasshopper   \n",
       "3        The Ass and His Masters   \n",
       "4      The Ass and his Purchaser   \n",
       "..                           ...   \n",
       "81           The Three Tradesmen   \n",
       "82     The Vixen and the Lioness   \n",
       "83          The Wolf and the Kid   \n",
       "84  The Wolf in Sheep's Clothing   \n",
       "85   The Woodman and the Serpent   \n",
       "\n",
       "                                              rel_url  \\\n",
       "0           /cgi/aesop1.cgi?sel&TheAntandtheChrysalis   \n",
       "1   /cgi/aesop1.cgi?sel&TheAntandtheDove&&antdove.ram   \n",
       "2   /cgi/aesop1.cgi?sel&TheAntandtheGrasshopper&&a...   \n",
       "3             /cgi/aesop1.cgi?sel&TheAssandHisMasters   \n",
       "4          /cgi/aesop1.cgi?sel&TheAssandhisPurchaser2   \n",
       "..                                                ...   \n",
       "81              /cgi/aesop1.cgi?sel&TheThreeTradesmen   \n",
       "82          /cgi/aesop1.cgi?sel&TheVixenandtheLioness   \n",
       "83               /cgi/aesop1.cgi?sel&TheWolfandtheKid   \n",
       "84       /cgi/aesop1.cgi?sel&TheWolfinSheepsClothing2   \n",
       "85        /cgi/aesop1.cgi?sel&TheWoodmanandtheSerpent   \n",
       "\n",
       "                                                  url  \n",
       "0   https://aesopfables.com//cgi/aesop1.cgi?sel&Th...  \n",
       "1   https://aesopfables.com//cgi/aesop1.cgi?sel&Th...  \n",
       "2   https://aesopfables.com//cgi/aesop1.cgi?sel&Th...  \n",
       "3   https://aesopfables.com//cgi/aesop1.cgi?sel&Th...  \n",
       "4   https://aesopfables.com//cgi/aesop1.cgi?sel&Th...  \n",
       "..                                                ...  \n",
       "81  https://aesopfables.com//cgi/aesop1.cgi?sel&Th...  \n",
       "82  https://aesopfables.com//cgi/aesop1.cgi?sel&Th...  \n",
       "83  https://aesopfables.com//cgi/aesop1.cgi?sel&Th...  \n",
       "84  https://aesopfables.com//cgi/aesop1.cgi?sel&Th...  \n",
       "85  https://aesopfables.com//cgi/aesop1.cgi?sel&Th...  \n",
       "\n",
       "[86 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rootdir = '/Users/thorwhalen/Dropbox/_od/generated/aesop_fables/'\n",
    "files = Files(rootdir)\n",
    "fable_table = get_fable_table(files)\n",
    "fable_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "from dol import TextFiles, wrap_kvs\n",
    "import oa.examples.illustrate_stories as ii\n",
    "\n",
    "\n",
    "def get_original_story(url):\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    return soup.find(\"pre\").text.strip()\n",
    "\n",
    "\n",
    "def get_original_stories(fable_table, original_stories: Mapping):\n",
    "    for title, url in zip(fable_table.title, fable_table.url):\n",
    "        original_story = get_original_story(url)\n",
    "        original_stories[title] = original_story\n",
    "\n",
    "\n",
    "def get_rhyming_stories(\n",
    "    original_stories: Mapping, *, rhyming_stories: Mapping, **kwargs\n",
    "):\n",
    "    for title, original_story in original_stories.items():\n",
    "        original_story = original_stories[title]\n",
    "        rhyming_story = ii.make_it_rhyming(original_story, **kwargs)\n",
    "        rhyming_stories[title] = rhyming_story\n",
    "\n",
    "\n",
    "# def get_story_illustrations(\n",
    "#     rhyming_stories: Mapping, *, story_illustrations: Mapping, **kwargs\n",
    "# ):\n",
    "#     ii.get_image_description()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Texts = wrap_kvs(\n",
    "    TextFiles, id_of_key=lambda x: x + '.txt', key_of_id=lambda x: x[:-len('.txt')]\n",
    ")\n",
    "original_stories = Texts(os.path.join(rootdir, 'elements', 'original_stories'))\n",
    "rhyming_stories = Texts(os.path.join(rootdir, 'elements', 'rhyming_stories'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_original_stories(fable_table, original_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_rhyming_stories(original_stories, rhyming_stories=rhyming_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = rhyming_stories['The Ant and the Dove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once an ant was running\n",
      "In the sunshine, it was funning\n",
      "Searching for food, it ran with glee\n",
      "When it saw a chrysalis, by a tree\n",
      "\n",
      "The chrysalis was moving its tail\n",
      "And caught the ant's attention without fail\n",
      "The ant saw the chrysalis was alive\n",
      "And was shocked, it was a surprise\n",
      "\n",
      "\"Oh, what a pitiable animal,\" said the ant\n",
      "\"Your fate is sad, you can't even chant\n",
      "I can run, climb, and reach the tallest tree\n",
      "While you are trapped in a shell, unable to be free\"\n",
      "\n",
      "The chrysalis didn't say a word\n",
      "It listened when the ant thought it was absurd\n",
      "Days passed, and the ant came again\n",
      "But found only an empty shell, and felt a pain\n",
      "\n",
      "Suddenly, it felt something above\n",
      "And saw a butterfly, a symbol of love\n",
      "It said, \"Behold in me, your much-pitied friend\n",
      "You can run and climb, but can't fly, that's the end\"\n",
      "\n",
      "So, the ant learned a lesson, you see\n",
      "Appearances are deceptive, let it be\n",
      "The butterfly flew away, it was gone\n",
      "The ant learned to appreciate others, it had won!\n"
     ]
    }
   ],
   "source": [
    "print(rhyming_stories['The Ant and the Chrysalis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T10:57:04.333833Z",
     "start_time": "2023-03-06T10:57:04.324059Z"
    }
   },
   "source": [
    "## illustrating concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T08:13:45.043119Z",
     "start_time": "2023-04-06T08:13:44.999792Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T08:13:57.670573Z",
     "start_time": "2023-04-06T08:13:45.045144Z"
    }
   },
   "outputs": [],
   "source": [
    "from oa.base import chatgpt, dalle\n",
    "from oa.examples.illustrate_stories import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T08:14:28.263296Z",
     "start_time": "2023-04-06T08:14:26.962195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Streamlined development processes  \n",
      " Strong focus on customer satisfaction \n",
      " Increased collaboration among team members\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(topic_points('The benefits of extreme programming'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T07:29:13.907185Z",
     "start_time": "2023-04-06T07:29:09.549507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Team-Oriented Approach': 'The Agile framework fosters collaboration between team members and promotes a project-oriented approach to problem solving.',\n",
       " 'Transparency & Flexibility': 'With Agile, progress and changes are visible to everyone involved, and scope is easily validatable or modified. ',\n",
       " 'Adaptable Environment': 'Agile provides a flexible and adaptive workspace, where teams can shift their plans quickly and easily when needed to suit the particular project.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "t = json.loads(\n",
    "    topic_points_json('The benefits of the Agile framework', max_n_words=30)\n",
    ")\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T07:20:12.500482Z",
     "start_time": "2023-04-06T07:20:07.809370Z"
    }
   },
   "outputs": [],
   "source": [
    "t = oa.chatgpt(prompt, max_tokens=2048, n=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T07:22:40.707793Z",
     "start_time": "2023-04-06T07:22:40.681354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Benefits of Extreme Programming',\n",
       " 'Increased Quality',\n",
       " 'Increased Efficiency']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "d = json.loads(t['choices'][0]['text'].strip()[1:-1])\n",
    "list(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-06T07:06:32.346551Z",
     "start_time": "2023-04-06T07:06:32.097672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['babbage',\n",
       " 'davinci',\n",
       " 'text-davinci-edit-001',\n",
       " 'babbage-code-search-code',\n",
       " 'text-similarity-babbage-001',\n",
       " 'code-davinci-edit-001',\n",
       " 'text-davinci-001',\n",
       " 'ada',\n",
       " 'babbage-code-search-text',\n",
       " 'babbage-similarity',\n",
       " 'code-search-babbage-text-001',\n",
       " 'text-curie-001',\n",
       " 'code-search-babbage-code-001',\n",
       " 'text-davinci-003',\n",
       " 'text-ada-001',\n",
       " 'text-embedding-ada-002',\n",
       " 'text-similarity-ada-001',\n",
       " 'curie-instruct-beta',\n",
       " 'ada-code-search-code',\n",
       " 'ada-similarity',\n",
       " 'code-search-ada-text-001',\n",
       " 'text-search-ada-query-001',\n",
       " 'davinci-search-document',\n",
       " 'ada-code-search-text',\n",
       " 'text-search-ada-doc-001',\n",
       " 'davinci-instruct-beta',\n",
       " 'text-similarity-curie-001',\n",
       " 'code-search-ada-code-001',\n",
       " 'ada-search-query',\n",
       " 'text-search-davinci-query-001',\n",
       " 'curie-search-query',\n",
       " 'davinci-search-query',\n",
       " 'babbage-search-document',\n",
       " 'ada-search-document',\n",
       " 'text-search-curie-query-001',\n",
       " 'text-search-babbage-doc-001',\n",
       " 'curie-search-document',\n",
       " 'gpt-3.5-turbo',\n",
       " 'whisper-1',\n",
       " 'text-search-curie-doc-001',\n",
       " 'babbage-search-query',\n",
       " 'text-babbage-001',\n",
       " 'text-search-davinci-doc-001',\n",
       " 'text-search-babbage-query-001',\n",
       " 'curie-similarity',\n",
       " 'gpt-3.5-turbo-0301',\n",
       " 'curie',\n",
       " 'text-similarity-davinci-001',\n",
       " 'text-davinci-002',\n",
       " 'davinci-similarity']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import oa.base\n",
    "oa.base.list_engine_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-07T10:35:52.050069Z",
     "start_time": "2023-03-07T10:35:52.040701Z"
    }
   },
   "outputs": [],
   "source": [
    "# openai.Engine.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T15:30:21.369584Z",
     "start_time": "2023-03-06T15:30:19.223966Z"
    }
   },
   "outputs": [],
   "source": [
    "profile = \"an illustrator who knows software principles\"\n",
    "\n",
    "description = \"\"\"which is a software development approach that emphasizes understanding and modeling the business \n",
    "domain to build software that is aligned with business needs.\"\"\"\n",
    "\n",
    "r = chatgpt(template.format(profile=profile, description=description))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T15:30:39.068263Z",
     "start_time": "2023-03-06T15:30:39.062471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6r752hwhpjQGtpHscV7ziNftomKqS at 0x11ab58e50> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" from the tiniest basis details to the specific application. A freelancer he\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1678116620,\n",
       "  \"id\": \"cmpl-6r752hwhpjQGtpHscV7ziNftomKqS\",\n",
       "  \"model\": \"davinci\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 7,\n",
       "    \"total_tokens\": 23\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T15:31:17.115888Z",
     "start_time": "2023-03-06T15:31:17.101504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from the tiniest basis details to the specific application. A freelancer he'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['choices'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## open ai tutorial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T10:57:54.392450Z",
     "start_time": "2023-03-06T10:57:49.165062Z"
    }
   },
   "outputs": [],
   "source": [
    "from oa.util import openai\n",
    "\n",
    "image_resp = openai.Image.create(\n",
    "    prompt=\"two dogs playing chess, oil painting\", n=2, size=\"512x512\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T10:58:51.634823Z",
     "start_time": "2023-03-06T10:58:51.622026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x1141da750> JSON: {\n",
       "  \"created\": 1678100274,\n",
       "  \"data\": [\n",
       "    {\n",
       "      \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-cZTRQYIOYLJ8PqjUK5A340FJ.png?st=2023-03-06T09%3A57%3A54Z&se=2023-03-06T11%3A57%3A54Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-03-06T02%3A38%3A13Z&ske=2023-03-07T02%3A38%3A13Z&sks=b&skv=2021-08-06&sig=PI0GdIDAZYbeZhUim5pndE3N0eiPGk83Wj/CDgJCKXQ%3D\"\n",
       "    },\n",
       "    {\n",
       "      \"url\": \"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-a3pizi7UpPlH4Zbuo0z3P0Jg.png?st=2023-03-06T09%3A57%3A54Z&se=2023-03-06T11%3A57%3A54Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-03-06T02%3A38%3A13Z&ske=2023-03-07T02%3A38%3A13Z&sks=b&skv=2021-08-06&sig=53H4WPi1XHmUkn96ud5K2ubbsfbHU/XN3JlaCSZGwEk%3D\"\n",
       "    }\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T15:15:57.145618Z",
     "start_time": "2023-03-06T15:15:56.888043Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_resp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimage_resp\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_resp' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T13:25:51.643336Z",
     "start_time": "2023-03-06T13:25:51.629327Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# r = requests.get(image_resp['data'][0]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T10:59:04.893668Z",
     "start_time": "2023-03-06T10:59:04.879908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-cZTRQYIOYLJ8PqjUK5A340FJ.png?st=2023-03-06T09%3A57%3A54Z&se=2023-03-06T11%3A57%3A54Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-03-06T02%3A38%3A13Z&ske=2023-03-07T02%3A38%3A13Z&sks=b&skv=2021-08-06&sig=PI0GdIDAZYbeZhUim5pndE3N0eiPGk83Wj/CDgJCKXQ%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=image_resp['data'][0]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-06T10:59:27.276883Z",
     "start_time": "2023-03-06T10:59:27.262490Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-a3pizi7UpPlH4Zbuo0z3P0Jg.png?st=2023-03-06T09%3A57%3A54Z&se=2023-03-06T11%3A57%3A54Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-03-06T02%3A38%3A13Z&ske=2023-03-07T02%3A38%3A13Z&sks=b&skv=2021-08-06&sig=53H4WPi1XHmUkn96ud5K2ubbsfbHU/XN3JlaCSZGwEk%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=image_resp['data'][1]['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chatGPTs code for illustrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T13:29:57.641436Z",
     "start_time": "2023-03-29T13:29:57.635971Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from oa.scrap.illustrating_stories import illustrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T13:30:17.888709Z",
     "start_time": "2023-03-29T13:30:17.877800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "it = illustrate(\"the ant and the grasshopper\", image_style='Salavadore Dali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T13:30:38.824563Z",
     "start_time": "2023-03-29T13:30:27.030731Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T13:31:12.149389Z",
     "start_time": "2023-03-29T13:31:12.140958Z"
    }
   },
   "outputs": [],
   "source": [
    "images = t['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T13:31:13.551225Z",
     "start_time": "2023-03-29T13:31:13.534754Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-4ircaau5ETaqAQBfH0BQKA9O.png?st=2023-03-29T12%3A30%3A34Z&se=2023-03-29T14%3A30%3A34Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-03-29T11%3A05%3A44Z&ske=2023-03-30T11%3A05%3A44Z&sks=b&skv=2021-08-06&sig=nIQoxjlWNa8cL2Q9Y2PcLI0BVAZ3IeHCXKZ7dY4rqM8%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(url=images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-29T13:31:16.473225Z",
     "start_time": "2023-03-29T13:31:16.453474Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-AY3lr3H3xB9yPQ0HGR498f9M/user-7ZNCDYLWzP0GT48V6DCiTFWt/img-8vWP8PD8u03CrGoYw2BWbzWt.png?st=2023-03-29T12%3A30%3A38Z&se=2023-03-29T14%3A30%3A38Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-03-29T11%3A42%3A53Z&ske=2023-03-30T11%3A42%3A53Z&sks=b&skv=2021-08-06&sig=GrEZBtd%2BtPt5aaxECx73xdtDs3LqaRam6jKhSRfACDI%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "2515.56px",
    "left": "223px",
    "top": "111.45px",
    "width": "173px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "5a05cf141187f787fb6e83e7d454331f1c13ec1ebc2819d212853f70d1e7de77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
