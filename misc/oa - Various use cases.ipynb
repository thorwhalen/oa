{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4o-mini'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from oa import DFLT_MODEL, prompt_json_function, prompt_function\n",
    "from lkj import wrapped_print\n",
    "\n",
    "use_cases_for_person = prompt_function(\n",
    "    \"\"\"Start by reflecting on a person named {person_name}, c\n",
    "    onsidering their academic background, work experience, \n",
    "    business expertise, current position, and personal interests. \n",
    "    Then, based on this understanding, generate five use cases of \n",
    "    generative AI that would most likely capture their interest. \n",
    "    For each use case, explain why it aligns with their background \n",
    "    and interests, ensuring that your reasoning is clear and follows \n",
    "    logically from your reflection on their profile.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = [\n",
    "    \"Jahrl Stefan Norberg\", \n",
    "]\n",
    "person_info = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for person in persons:\n",
    "    person_info[person] = use_cases_for_person(person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Reflection on Jahrl Stefan Norberg:**\n",
      "\n",
      "1. **Academic Background:** Jahrl likely possesses a strong academic foundation,\n",
      " potentially in fields such as business, technology, or a related discipline. \n",
      "This background would have equipped him with analytical skills and a solid \n",
      "understanding of business principles.\n",
      "\n",
      "2. **Work Experience:** His work experience may encompass roles in business \n",
      "management, consulting, or technology, suggesting familiarity with both \n",
      "operational and strategic aspects of organizations. This experience could inform\n",
      " his appreciation for innovative solutions that enhance productivity or drive \n",
      "business growth.\n",
      "\n",
      "3. **Business Expertise:** Jahrl’s expertise could be in areas such as \n",
      "entrepreneurship, project management, or digital transformation, especially if \n",
      "he has worked on initiatives that embrace technological advancements. This \n",
      "expertise would resonate with trends in generative AI, particularly in improving\n",
      " operational efficiency and decision-making.\n",
      "\n",
      "4. **Current Position:** Depending on his current role, Jahrl might be involved \n",
      "in leadership, innovation, or strategic development. If he’s in a position of \n",
      "influence, he likely seeks out technologies that can disrupt standard practices \n",
      "and offer competitive advantages.\n",
      "\n",
      "5. **Personal Interests:** Jahrl may have personal interests in technology, \n",
      "creative processes, or enabling organizational growth. If he is passionate about\n",
      " leveraging technology for better outcomes, that would be crucial in tailoring \n",
      "the use cases for generative AI that would appeal to him.\n",
      "\n",
      "**Use Cases of Generative AI:**\n",
      "\n",
      "1. **Automated Business Reporting and Analysis:**\n",
      "   - **Why it aligns:** This use case would appeal to Jahrl’s experience in \n",
      "managing operations and his interest in data-driven decision-making. Generative \n",
      "AI can automate the synthesis of various data sources into cohesive reports, \n",
      "saving time and improving strategic insight.\n",
      "\n",
      "2. **Content Generation for Marketing:**\n",
      "   - **Why it aligns:** If Jahrl has experience in marketing or outreach, the \n",
      "use of generative AI to create engaging content, social media posts, or email \n",
      "campaigns would facilitate his efforts to connect with clients or stakeholders. \n",
      "It plays into his interest in innovative marketing strategies.\n",
      "\n",
      "3. **Personalized Learning Platforms for Employee Development:**\n",
      "   - **Why it aligns:** As a business leader, Jahrl may value employee growth \n",
      "and training. Generative AI can create personalized learning paths for \n",
      "employees, addressing their individual strengths and weaknesses, which would \n",
      "help in enhancing team performance and satisfaction.\n",
      "\n",
      "4. **Brainstorming and Ideation Tools:**\n",
      "   - **Why it aligns:** If Jahrl is involved in innovation or product \n",
      "development, generative AI tools that assist in brainstorming could help \n",
      "generate new ideas or business strategies efficiently. This synthesis of \n",
      "creativity and technology would likely fascinate him.\n",
      "\n",
      "5. **Customer Support Chatbots with Advanced AI Capabilities:**\n",
      "   - **Why it aligns:** Jahrl may be interested in enhancing customer experience\n",
      " and operational efficiency. AI-powered chatbots can handle inquiries round the \n",
      "clock, providing consistent support and freeing up human resources for more \n",
      "complex tasks—aligning with his seek for innovative solutions to boost business \n",
      "functionality.\n",
      "\n",
      "In summary, the use cases suggested hinge on the possibility that Jahrl Stefan \n",
      "Norberg has a solid grounding in business and technology, with potential roles \n",
      "that seek efficiency, innovation, and strategic advantage. Generative AI \n",
      "applications aligned with these areas would not only catch his interest but also\n",
      " provide substantial value to his work and aspirations.\n"
     ]
    }
   ],
   "source": [
    "wrapped_print(person_info[\"Jahrl Stefan Norberg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Reflection on Thor Whalen\n",
      "\n",
      "**Academic Background**: Assuming Thor Whalen has a strong academic foundation \n",
      "in business administration, computer science, or a related field. This \n",
      "background will offer him a profound understanding of both business strategies \n",
      "and technological advancements, laying a great foundation for applying \n",
      "generative AI.\n",
      "\n",
      "**Work Experience**: With potential experience in technology consulting or \n",
      "digital transformation, Thor is likely adept at managing complex projects and \n",
      "understanding the interplay between technology and business outcomes. His \n",
      "previous roles might include working with startups or established tech firms, \n",
      "equipping him with insights into market trends and operational efficiencies.\n",
      "\n",
      "**Business Expertise**: Thor's expertise could range from entrepreneurship to \n",
      "corporate strategy. This would suggest that he possesses the skills to identify \n",
      "opportunities for innovation and improvement within business processes, making \n",
      "him a valuable resource for companies looking to leverage AI technologies.\n",
      "\n",
      "**Current Position**: If Thor is currently in a leadership role, such as a \n",
      "product manager or a chief technology officer, he would be responsible for \n",
      "shaping the strategic direction of AI initiatives in his organization. This \n",
      "position demands not only technical knowledge but also the ability to \n",
      "communicate effectively with diverse stakeholders.\n",
      "\n",
      "**Personal Interests**: Assuming Thor has an interest in emerging technologies, \n",
      "entrepreneurship, and perhaps even social impact initiatives, he is likely to be\n",
      " enthusiastic about developing solutions that address real-world problems \n",
      "through innovative approaches.\n",
      "\n",
      "### Use Cases of Generative AI\n",
      "\n",
      "1. **Automated Market Research Generation**:\n",
      "   - **Reasoning**: Thor’s background in business and technology positions him \n",
      "to appreciate the value of real-time market insights. Leveraging generative AI \n",
      "to automate market research processes aligns perfectly with his expertise. It \n",
      "would allow Thor to quickly analyze consumer trends and preferences, thus aiding\n",
      " in more informed decision-making.\n",
      "\n",
      "2. **AI-Powered Product Development**:\n",
      "   - **Reasoning**: If Thor is overseeing product development, using generative \n",
      "AI to create prototypes or simulate user interactions can significantly \n",
      "accelerate the innovation cycle. This use case resonates with his potential role\n",
      " in technology strategy and aligns with his interest in entrepreneurship by \n",
      "fostering rapid iteration of ideas.\n",
      "\n",
      "3. **Content Creation for Brand Storytelling**:\n",
      "   - **Reasoning**: A leadership role in marketing or brand management would \n",
      "benefit from generative AI tools that create compelling content. Thor’s \n",
      "understanding of business communication could help frame the use of AI in \n",
      "telling engaging brand stories, enhancing both customer engagement and brand \n",
      "loyalty.\n",
      "\n",
      "4. **Personalized Customer Experiences**:\n",
      "   - **Reasoning**: The ability to tailor customer interactions through \n",
      "generative AI is crucial for successful modern businesses. Thor’s work \n",
      "experience positions him to advocate for AI solutions that adapt offerings based\n",
      " on user behavior, enhancing customer satisfaction and driving sales—key \n",
      "objectives for any business leader.\n",
      "\n",
      "5. **AI-Driven Decision Support Systems**:\n",
      "   - **Reasoning**: Generative AI can play a central role in creating \n",
      "sophisticated decision support tools that synthesize data for strategic business\n",
      " planning. For someone in Thor's role, utilizing these systems can streamline \n",
      "operations and lead to better outcomes, making them an appealing tool for both \n",
      "him and his organization.\n",
      "\n",
      "These use cases demonstrate how Thor Whalen's academic, professional, and \n",
      "personal attributes align with the potential applications of generative AI, \n",
      "making them relevant and impactful in both his career and interests.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = use_cases_for_person('Thor Whalen')\n",
    "wrapped_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import openai\n",
    "import os \n",
    "\n",
    "import ug\n",
    "\n",
    "# Example using Google Custom Search API\n",
    "search_api_key = os.environ['GOOGLE_API_KEY']\n",
    "search_engine_id = \"9794ddc10ad374fbd\"\n",
    "query = \"latest research on AI ethics\"\n",
    "\n",
    "search_url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={search_api_key}&cx={search_engine_id}\"\n",
    "search_response = requests.get(search_url).json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Callable\n",
    "\n",
    "from config2py import simple_config_getter\n",
    "import requests\n",
    "\n",
    "config_getter = simple_config_getter('ug')\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def google_search(\n",
    "    query: str,\n",
    "    egress: Callable = identity,\n",
    "    *,\n",
    "    search_api_key=None,\n",
    "    search_engine_id=None,\n",
    "):\n",
    "    search_api_key = config_getter('GOOGLE_API_KEY')\n",
    "    search_engine_id = config_getter('GOOGLE_SEARCH_ENGINE_ID')\n",
    "    search_url = f\"https://www.googleapis.com/customsearch/v1?q={query}&key={search_api_key}&cx={search_engine_id}\"\n",
    "    search_response = requests.get(search_url).json()\n",
    "    return search_response\n",
    "\n",
    "\n",
    "def extract_item_fields(fields=('title', 'link')):\n",
    "    from glom import glom\n",
    "\n",
    "    return partial(glom, spec=('items', [{field: field for field in fields}]))\n",
    "\n",
    "google_search.titles_and_links = partial(\n",
    "    google_search, egress=extract_item_fields(['title', 'link'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = google_search('Thor Whalen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'customsearch#result',\n",
       " 'title': 'Thor Whalen - Figiri | LinkedIn',\n",
       " 'htmlTitle': '<b>Thor Whalen</b> - Figiri | LinkedIn',\n",
       " 'link': 'https://uk.linkedin.com/in/thorwhalen',\n",
       " 'displayLink': 'uk.linkedin.com',\n",
       " 'snippet': 'I am a seasoned Director of Machine Learning with a passion for developing cutting-edge… · Experience: Figiri · Education: Emory University · Location:\\xa0...',\n",
       " 'htmlSnippet': 'I am a seasoned Director of Machine Learning with a passion for developing cutting-edge… · Experience: Figiri · Education: Emory University · Location:&nbsp;...',\n",
       " 'formattedUrl': 'https://uk.linkedin.com/in/thorwhalen',\n",
       " 'htmlFormattedUrl': 'https://uk.linkedin.com/in/<b>thorwhalen</b>',\n",
       " 'pagemap': {'cse_thumbnail': [{'src': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRULGNTt0-Lk7Qa0I20pF9oLRcTCKcI0e6mPFc5piiXlUYzfoVc-U5PfNn3&s',\n",
       "    'width': '297',\n",
       "    'height': '170'}],\n",
       "  'metatags': [{'og:image': 'https://media.licdn.com/dms/image/v2/C4E03AQE6wgUzTJf2Qw/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1516307985303?e=2147483647&v=beta&t=Q12zQEJaqwhaAm68DYtzIc01IJ4Wkn9nVyXgMPkZO6I',\n",
       "    'twitter:card': 'summary',\n",
       "    'linkedin:pagetag': 'openToProvider',\n",
       "    'platform-worker': 'https://static.licdn.com/aero-v1/sc/h/7nirg34a8ey4y2l4rw7xgwxx4',\n",
       "    'al:android:package': 'com.linkedin.android',\n",
       "    'bingbot': 'nocache',\n",
       "    'locale': 'en_US',\n",
       "    'al:ios:url': 'https://uk.linkedin.com/in/thorwhalen',\n",
       "    'og:description': 'I am a seasoned Director of Machine Learning with a passion for developing cutting-edge… · Experience: Figiri · Education: Emory University · Location: London · 500+ connections on LinkedIn. View Thor Whalen’s profile on LinkedIn, a professional community of 1 billion members.',\n",
       "    'al:ios:app_store_id': '288429040',\n",
       "    'platform': 'https://static.licdn.com/aero-v1/sc/h/cbkh0sx42xkppqhywbu843ar7',\n",
       "    'twitter:image': 'https://media.licdn.com/dms/image/v2/C4E03AQE6wgUzTJf2Qw/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1516307985303?e=2147483647&v=beta&t=Q12zQEJaqwhaAm68DYtzIc01IJ4Wkn9nVyXgMPkZO6I',\n",
       "    'profile:last_name': 'Whalen',\n",
       "    'twitter:site': '@Linkedin',\n",
       "    'litmsprofilename': 'public-profile-frontend',\n",
       "    'profile:first_name': 'Thor',\n",
       "    'og:type': 'profile',\n",
       "    'twitter:title': 'Thor Whalen - Figiri | LinkedIn',\n",
       "    'al:ios:app_name': 'LinkedIn',\n",
       "    'og:title': 'Thor Whalen - Figiri | LinkedIn',\n",
       "    'pagekey': 'public_profile_v3_mobile',\n",
       "    'al:android:url': 'https://uk.linkedin.com/in/thorwhalen',\n",
       "    'viewport': 'width=device-width, initial-scale=1.0',\n",
       "    'twitter:description': 'I am a seasoned Director of Machine Learning with a passion for developing cutting-edge… · Experience: Figiri · Education: Emory University · Location: London · 500+ connections on LinkedIn. View Thor Whalen’s profile on LinkedIn, a professional community of 1 billion members.',\n",
       "    'ubba': 'https://static.licdn.com/aero-v1/sc/h/6q1rjhrayszfmd8hoxlh4rx2o',\n",
       "    'og:url': 'https://uk.linkedin.com/in/thorwhalen',\n",
       "    'al:android:app_name': 'LinkedIn'}],\n",
       "  'cse_image': [{'src': 'https://media.licdn.com/dms/image/sync/v2/D4D27AQEi0okyp3C82w/articleshare-shrink_800/articleshare-shrink_800/0/1721907427582?e=2147483647&v=beta&t=NG7XBFwpEw7NKnXTrcavXZhmZDUItYIZ9ahO3Yoi8uA'}],\n",
       "  'Person': [{}]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['items'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'New Gift Supports Research in AI, Ethics',\n",
       "  'link': 'https://www.duq.edu/news-and-stories/releases/grefenstette-hillman-gift.php'},\n",
       " {'title': 'Ethics of Artificial Intelligence | UNESCO',\n",
       "  'link': 'https://www.unesco.org/en/artificial-intelligence/recommendation-ethics'},\n",
       " {'title': 'Ethical concerns mount as AI takes bigger decision-making role ...',\n",
       "  'link': 'https://news.harvard.edu/gazette/story/2020/10/ethical-concerns-mount-as-ai-takes-bigger-decision-making-role/'},\n",
       " {'title': 'NEH Awards $2.72 Million to Create Research Centers Examining ...',\n",
       "  'link': 'https://www.neh.gov/news/neh-awards-272-million-create-ai-research-centers'},\n",
       " {'title': 'Ethics and Artificial Intelligence | Stanford HAI',\n",
       "  'link': 'https://hai.stanford.edu/ethics-and-artificial-intelligence'},\n",
       " {'title': 'The Ethics in AI Institute | Humanities | University of Oxford',\n",
       "  'link': 'https://www.schwarzmancentre.ox.ac.uk/ethicsinai'},\n",
       " {'title': 'What is AI Ethics? | IBM',\n",
       "  'link': 'https://www.ibm.com/topics/ai-ethics'},\n",
       " {'title': 'Specific challenges posed by artificial intelligence in research ethics ...',\n",
       "  'link': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10358356/'},\n",
       " {'title': 'NIST Researchers Suggest Historical Precedent for Ethical AI ...',\n",
       "  'link': 'https://www.nist.gov/news-events/news/2024/02/nist-researchers-suggest-historical-precedent-ethical-ai-research'},\n",
       " {'title': 'NSF announces 7 new National Artificial Intelligence Research ...',\n",
       "  'link': 'https://new.nsf.gov/news/nsf-announces-7-new-national-artificial'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glom import glom\n",
    "\n",
    "\n",
    "# Glom spec to extract titles and links\n",
    "spec = ('items', [{'title': 'title', 'link': 'link'}])\n",
    "\n",
    "# Extract titles and links using glom\n",
    "extracted_data = glom(search_response, spec)\n",
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Unknown parameter: 'response_format.json_schema.type'.\", 'type': 'invalid_request_error', 'param': 'response_format.json_schema.type', 'code': 'unknown_parameter'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01moa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m prompt_json_function\n\u001b[0;32m----> 3\u001b[0m \u001b[43mprompt_json_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGive me a one sentence joke\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dropbox/py/proj/t/oa/oa/tools.py:298\u001b[0m, in \u001b[0;36mprompt_function.<locals>.ask_oa\u001b[0;34m(*ask_oa_args, **ask_oa_kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;129m@func_wrap\u001b[39m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mask_oa\u001b[39m(\u001b[38;5;241m*\u001b[39mask_oa_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mask_oa_kwargs):\n\u001b[1;32m    297\u001b[0m     embodied_template \u001b[38;5;241m=\u001b[39m embody_prompt(\u001b[38;5;241m*\u001b[39mask_oa_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mask_oa_kwargs)\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m egress(\u001b[43mprompt_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43membodied_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprompt_func_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Dropbox/py/proj/t/oa/oa/base.py:115\u001b[0m, in \u001b[0;36mchat\u001b[0;34m(prompt, model, messages, **chat_params)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@Sig\u001b[39m\u001b[38;5;241m.\u001b[39mreplace_kwargs_using(_raw_chat)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat\u001b[39m(prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, model\u001b[38;5;241m=\u001b[39mDFLT_MODEL, messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchat_params):\n\u001b[0;32m--> 115\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43m_raw_chat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchat_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# TODO: Make attr and item getters more robust (use glom?)\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Dropbox/py/proj/t/oa/oa/base.py:105\u001b[0m, in \u001b[0;36m_raw_chat\u001b[0;34m(prompt, model, messages, **chat_params)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m: prompt}]\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mchat_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/openai/resources/chat/completions.py:668\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    667\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/openai/_base_client.py:1260\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1248\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1257\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1258\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1259\u001b[0m     )\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/openai/_base_client.py:937\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    930\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    935\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    936\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.13/envs/p10/lib/python3.10/site-packages/openai/_base_client.py:1041\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1038\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1040\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1044\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1045\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mget_max_retries(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries) \u001b[38;5;241m-\u001b[39m retries,\n\u001b[1;32m   1050\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Unknown parameter: 'response_format.json_schema.type'.\", 'type': 'invalid_request_error', 'param': 'response_format.json_schema.type', 'code': 'unknown_parameter'}}"
     ]
    }
   ],
   "source": [
    "from oa import prompt_json_function\n",
    "\n",
    "prompt_json_function(\"Give me a one sentence joke\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
